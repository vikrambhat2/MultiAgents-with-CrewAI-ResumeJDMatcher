{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4111e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import json\n",
    "from typing import Dict\n",
    "from dotenv import load_dotenv\n",
    "from crewai import Agent, Crew, Task, LLM, Process\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7c0dfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Crew, Task, LLM, Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92843951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ResumeParsingAgent import ResumeParsingAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a5502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "987fae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_model = LLM(\n",
    "    model=\"groq/meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7e6d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:09:14 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \n",
      "\n",
      "2025-06-05 14:09:14,550 - DEBUG - \n",
      "\n",
      "\u001b[92m14:09:14 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "2025-06-05 14:09:14,552 - DEBUG - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m14:09:14 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \u001b[92mlitellm.completion(model='groq/meta-llama/llama-4-scout-17b-16e-instruct', messages=[{'role': 'user', 'content': 'how to make a cup of coffee'}], temperature=0.7, stream=False)\u001b[0m\n",
      "2025-06-05 14:09:14,552 - DEBUG - \u001b[92mlitellm.completion(model='groq/meta-llama/llama-4-scout-17b-16e-instruct', messages=[{'role': 'user', 'content': 'how to make a cup of coffee'}], temperature=0.7, stream=False)\u001b[0m\n",
      "\u001b[92m14:09:14 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \n",
      "\n",
      "2025-06-05 14:09:14,553 - DEBUG - \n",
      "\n",
      "\u001b[92m14:09:14 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:458 - self.optional_params: {}\n",
      "2025-06-05 14:09:14,554 - DEBUG - self.optional_params: {}\n",
      "\u001b[92m14:09:14 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "2025-06-05 14:09:14,555 - DEBUG - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m14:09:14 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-scout-17b-16e-instruct; provider = groq\n",
      "2025-06-05 14:09:14,582 - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-scout-17b-16e-instruct; provider = groq\n",
      "\u001b[92m14:09:14 - LiteLLM:DEBUG\u001b[0m: utils.py:2908 - \n",
      "LiteLLM: Params passed to completion() {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'functions': None, 'function_call': None, 'temperature': 0.7, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'how to make a cup of coffee'}], 'thinking': None, 'web_search_options': None}\n",
      "2025-06-05 14:09:14,587 - DEBUG - \n",
      "LiteLLM: Params passed to completion() {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'functions': None, 'function_call': None, 'temperature': 0.7, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'how to make a cup of coffee'}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m14:09:14 - LiteLLM:DEBUG\u001b[0m: utils.py:2911 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.7, 'stream': False}\n",
      "2025-06-05 14:09:14,592 - DEBUG - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.7, 'stream': False}\n",
      "\u001b[92m14:09:14 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - Final returned optional params: {'temperature': 0.7, 'stream': False, 'extra_body': {}}\n",
      "2025-06-05 14:09:14,593 - DEBUG - Final returned optional params: {'temperature': 0.7, 'stream': False, 'extra_body': {}}\n",
      "\u001b[92m14:09:14 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:458 - self.optional_params: {'temperature': 0.7, 'stream': False, 'extra_body': {}}\n",
      "2025-06-05 14:09:14,594 - DEBUG - self.optional_params: {'temperature': 0.7, 'stream': False, 'extra_body': {}}\n",
      "\u001b[92m14:09:14 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:901 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.groq.com/openai/v1/chat/completions \\\n",
      "-H 'Authorization: Be****An' -H 'Content-Type: ap****on' \\\n",
      "-d '{'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'messages': [{'role': 'user', 'content': 'how to make a cup of coffee'}], 'temperature': 0.7, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "2025-06-05 14:09:14,595 - DEBUG - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.groq.com/openai/v1/chat/completions \\\n",
      "-H 'Authorization: Be****An' -H 'Content-Type: ap****on' \\\n",
      "-d '{'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'messages': [{'role': 'user', 'content': 'how to make a cup of coffee'}], 'temperature': 0.7, 'stream': False}'\n",
      "\u001b[0m\n",
      "\n",
      "2025-06-05 14:09:14,614 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "2025-06-05 14:09:14,678 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x30e5d5b90>\n",
      "2025-06-05 14:09:14,678 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1202f6960> server_hostname='api.groq.com' timeout=600.0\n",
      "2025-06-05 14:09:14,705 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x30e575e90>\n",
      "2025-06-05 14:09:14,705 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2025-06-05 14:09:14,706 - DEBUG - send_request_headers.complete\n",
      "2025-06-05 14:09:14,707 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-06-05 14:09:14,707 - DEBUG - send_request_body.complete\n",
      "2025-06-05 14:09:14,707 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-06-05 14:09:16,899 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 05 Jun 2025 13:09:16 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-europe-west3'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'999'), (b'x-ratelimit-remaining-tokens', b'29989'), (b'x-ratelimit-reset-requests', b'1m26.4s'), (b'x-ratelimit-reset-tokens', b'22ms'), (b'x-request-id', b'req_01jx03bgvvet6bpe1p4ynty0ne'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=7cmXBLsjEwGzZ7nyG_z6iBYgosJEvC0QWS7gPUsAHD4-1749128956-1.0.1.1-965s9UuiXg42GqlUse7e0HDXZQa5D8HnQSqm_fbOBnOU6.oGgTFiP48ArNtylAYoON4QMvAbiIILCx2B9LkmYBxpMfziHUcusbrnurAxzlc; path=/; expires=Thu, 05-Jun-25 13:39:16 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94afe73f09dc7a91-LHR'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2025-06-05 14:09:16,902 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 14:09:16,905 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-06-05 14:09:16,907 - DEBUG - receive_response_body.complete\n",
      "2025-06-05 14:09:16,908 - DEBUG - response_closed.started\n",
      "2025-06-05 14:09:16,908 - DEBUG - response_closed.complete\n",
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-40a94cb0-ff6e-4fbb-85ae-55017aa893b9\", \"object\": \"chat.completion\", \"created\": 1749128954, \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"A simple yet wonderful question! Making a cup of coffee can be a straightforward process, and I'm happy to guide you through it. Here's a step-by-step guide:\\n\\n**Method 1: Drip Coffee Maker**\\n\\n1. **Get your coffee beans**: Choose your favorite coffee beans or ground coffee. If you're using whole beans, grind them to a medium-coarse grind.\\n2. **Measure the coffee**: Use 1 tablespoon of coffee for every 6 ounces of water. For a standard cup, use about 10-12 grams of coffee.\\n3. **Add coffee to the filter**: Place a paper filter in the drip coffee maker and add the measured coffee grounds.\\n4. **Add water**: Fill the water reservoir with fresh, cold water. The recommended water temperature is between 195\\u00b0F and 205\\u00b0F.\\n5. **Turn on the coffee maker**: Switch on the coffee maker and let it do its magic.\\n6. **Wait and pour**: Wait for the coffee to brew (usually around 5-10 minutes). Once it's done, pour the coffee into your cup and enjoy!\\n\\n**Method 2: French Press**\\n\\n1. **Get your coffee beans**: Choose your favorite coffee beans or ground coffee. If you're using whole beans, grind them to a coarse grind.\\n2. **Measure the coffee**: Use 1 tablespoon of coffee for every 4 ounces of water. For a standard cup, use about 15-18 grams of coffee.\\n3. **Add coffee to the French Press**: Pour the measured coffee grounds into the French Press.\\n4. **Add water**: Pour hot water (around 200\\u00b0F) over the coffee grounds. Make sure the water is not boiling.\\n5. **Steep the coffee**: Let the coffee steep for 3-5 minutes, depending on your desired strength and flavor.\\n6. **Press and pour**: Press the plunger down slowly to separate the coffee grounds from the liquid. Pour the coffee into your cup and enjoy!\\n\\n**Method 3: Espresso Machine**\\n\\n1. **Get your coffee beans**: Choose your favorite coffee beans or ground coffee. If you're using whole beans, grind them to a fine grind.\\n2. **Measure the coffee**: Use about 14-17 grams of coffee for a double shot.\\n3. **Load the portafilter**: Place the measured coffee grounds into the portafilter and tamp it firmly.\\n4. **Attach the portafilter**: Attach the portafilter to the espresso machine's group head.\\n5. **Pull the shot**: Place a cup under the spout and close the machine's lever. The espresso machine will force pressurized hot water through the coffee grounds, creating a rich and concentrated shot of coffee.\\n6. **Enjoy your espresso**: Your espresso is now ready to drink. You can enjoy it on its own or use it as a base for other coffee drinks like lattes or cappuccinos.\\n\\n**Method 4: Pour-over**\\n\\n1. **Get your coffee beans**: Choose your favorite coffee beans or ground coffee. If you're using whole beans, grind them to a medium-coarse grind.\\n2. **Measure the coffee**: Use 1 tablespoon of coffee for every 6 ounces of water. For a standard cup, use about 10-12 grams of coffee.\\n3. **Place the filter**: Put a paper filter in the pour-over dripper.\\n4. **Add coffee**: Pour the measured coffee grounds into the filter.\\n5. **Add water**: Pour hot water (around 200\\u00b0F) over the coffee grounds in a circular motion.\\n6. **Wait and pour**: Allow the coffee to drip through the filter (usually around 3-4 minutes). Once it's done, pour the coffee into your cup and enjoy!\\n\\nThese are just a few common methods for making a cup of coffee. Feel free to experiment with different techniques, coffee beans, and ratios to find your perfect cup!\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.230102543, \"prompt_tokens\": 17, \"prompt_time\": 0.002829979, \"completion_tokens\": 796, \"completion_time\": 1.906970649, \"total_tokens\": 813, \"total_time\": 1.909800628}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_37da608fc1\", \"x_groq\": {\"id\": \"req_01jx03bgvvet6bpe1p4ynty0ne\"}}\n",
      "\n",
      "\n",
      "2025-06-05 14:09:16,909 - DEBUG - RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-40a94cb0-ff6e-4fbb-85ae-55017aa893b9\", \"object\": \"chat.completion\", \"created\": 1749128954, \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"A simple yet wonderful question! Making a cup of coffee can be a straightforward process, and I'm happy to guide you through it. Here's a step-by-step guide:\\n\\n**Method 1: Drip Coffee Maker**\\n\\n1. **Get your coffee beans**: Choose your favorite coffee beans or ground coffee. If you're using whole beans, grind them to a medium-coarse grind.\\n2. **Measure the coffee**: Use 1 tablespoon of coffee for every 6 ounces of water. For a standard cup, use about 10-12 grams of coffee.\\n3. **Add coffee to the filter**: Place a paper filter in the drip coffee maker and add the measured coffee grounds.\\n4. **Add water**: Fill the water reservoir with fresh, cold water. The recommended water temperature is between 195\\u00b0F and 205\\u00b0F.\\n5. **Turn on the coffee maker**: Switch on the coffee maker and let it do its magic.\\n6. **Wait and pour**: Wait for the coffee to brew (usually around 5-10 minutes). Once it's done, pour the coffee into your cup and enjoy!\\n\\n**Method 2: French Press**\\n\\n1. **Get your coffee beans**: Choose your favorite coffee beans or ground coffee. If you're using whole beans, grind them to a coarse grind.\\n2. **Measure the coffee**: Use 1 tablespoon of coffee for every 4 ounces of water. For a standard cup, use about 15-18 grams of coffee.\\n3. **Add coffee to the French Press**: Pour the measured coffee grounds into the French Press.\\n4. **Add water**: Pour hot water (around 200\\u00b0F) over the coffee grounds. Make sure the water is not boiling.\\n5. **Steep the coffee**: Let the coffee steep for 3-5 minutes, depending on your desired strength and flavor.\\n6. **Press and pour**: Press the plunger down slowly to separate the coffee grounds from the liquid. Pour the coffee into your cup and enjoy!\\n\\n**Method 3: Espresso Machine**\\n\\n1. **Get your coffee beans**: Choose your favorite coffee beans or ground coffee. If you're using whole beans, grind them to a fine grind.\\n2. **Measure the coffee**: Use about 14-17 grams of coffee for a double shot.\\n3. **Load the portafilter**: Place the measured coffee grounds into the portafilter and tamp it firmly.\\n4. **Attach the portafilter**: Attach the portafilter to the espresso machine's group head.\\n5. **Pull the shot**: Place a cup under the spout and close the machine's lever. The espresso machine will force pressurized hot water through the coffee grounds, creating a rich and concentrated shot of coffee.\\n6. **Enjoy your espresso**: Your espresso is now ready to drink. You can enjoy it on its own or use it as a base for other coffee drinks like lattes or cappuccinos.\\n\\n**Method 4: Pour-over**\\n\\n1. **Get your coffee beans**: Choose your favorite coffee beans or ground coffee. If you're using whole beans, grind them to a medium-coarse grind.\\n2. **Measure the coffee**: Use 1 tablespoon of coffee for every 6 ounces of water. For a standard cup, use about 10-12 grams of coffee.\\n3. **Place the filter**: Put a paper filter in the pour-over dripper.\\n4. **Add coffee**: Pour the measured coffee grounds into the filter.\\n5. **Add water**: Pour hot water (around 200\\u00b0F) over the coffee grounds in a circular motion.\\n6. **Wait and pour**: Allow the coffee to drip through the filter (usually around 3-4 minutes). Once it's done, pour the coffee into your cup and enjoy!\\n\\nThese are just a few common methods for making a cup of coffee. Feel free to experiment with different techniques, coffee beans, and ratios to find your perfect cup!\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.230102543, \"prompt_tokens\": 17, \"prompt_time\": 0.002829979, \"completion_tokens\": 796, \"completion_time\": 1.906970649, \"total_tokens\": 813, \"total_time\": 1.909800628}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_37da608fc1\", \"x_groq\": {\"id\": \"req_01jx03bgvvet6bpe1p4ynty0ne\"}}\n",
      "\n",
      "\n",
      "\u001b[92m14:09:16 - LiteLLM:INFO\u001b[0m: utils.py:1211 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-05 14:09:16,910 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1346 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "2025-06-05 14:09:16,912 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m14:09:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:09:16,912 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:09:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:09:16,913 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:09:16,914 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:09:16,914 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 1.87e-06, completion_tokens_cost_usd_dollar: 0.00027064\n",
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 1.87e-06, completion_tokens_cost_usd_dollar: 0.00027064\n",
      "2025-06-05 14:09:16,915 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 1.87e-06, completion_tokens_cost_usd_dollar: 0.00027064\n",
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.00027251\n",
      "2025-06-05 14:09:16,916 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 1.87e-06, completion_tokens_cost_usd_dollar: 0.00027064\n",
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.00027251\n",
      "2025-06-05 14:09:16,916 - DEBUG - response_cost: 0.00027251\n",
      "2025-06-05 14:09:16,916 - DEBUG - response_cost: 0.00027251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A simple yet wonderful question! Making a cup of coffee can be a straightforward process, and I'm happy to guide you through it. Here's a step-by-step guide:\\n\\n**Method 1: Drip Coffee Maker**\\n\\n1. **Get your coffee beans**: Choose your favorite coffee beans or ground coffee. If you're using whole beans, grind them to a medium-coarse grind.\\n2. **Measure the coffee**: Use 1 tablespoon of coffee for every 6 ounces of water. For a standard cup, use about 10-12 grams of coffee.\\n3. **Add coffee to the filter**: Place a paper filter in the drip coffee maker and add the measured coffee grounds.\\n4. **Add water**: Fill the water reservoir with fresh, cold water. The recommended water temperature is between 195째F and 205째F.\\n5. **Turn on the coffee maker**: Switch on the coffee maker and let it do its magic.\\n6. **Wait and pour**: Wait for the coffee to brew (usually around 5-10 minutes). Once it's done, pour the coffee into your cup and enjoy!\\n\\n**Method 2: French Press**\\n\\n1. **Get your coffee beans**: Choose your favorite coffee beans or ground coffee. If you're using whole beans, grind them to a coarse grind.\\n2. **Measure the coffee**: Use 1 tablespoon of coffee for every 4 ounces of water. For a standard cup, use about 15-18 grams of coffee.\\n3. **Add coffee to the French Press**: Pour the measured coffee grounds into the French Press.\\n4. **Add water**: Pour hot water (around 200째F) over the coffee grounds. Make sure the water is not boiling.\\n5. **Steep the coffee**: Let the coffee steep for 3-5 minutes, depending on your desired strength and flavor.\\n6. **Press and pour**: Press the plunger down slowly to separate the coffee grounds from the liquid. Pour the coffee into your cup and enjoy!\\n\\n**Method 3: Espresso Machine**\\n\\n1. **Get your coffee beans**: Choose your favorite coffee beans or ground coffee. If you're using whole beans, grind them to a fine grind.\\n2. **Measure the coffee**: Use about 14-17 grams of coffee for a double shot.\\n3. **Load the portafilter**: Place the measured coffee grounds into the portafilter and tamp it firmly.\\n4. **Attach the portafilter**: Attach the portafilter to the espresso machine's group head.\\n5. **Pull the shot**: Place a cup under the spout and close the machine's lever. The espresso machine will force pressurized hot water through the coffee grounds, creating a rich and concentrated shot of coffee.\\n6. **Enjoy your espresso**: Your espresso is now ready to drink. You can enjoy it on its own or use it as a base for other coffee drinks like lattes or cappuccinos.\\n\\n**Method 4: Pour-over**\\n\\n1. **Get your coffee beans**: Choose your favorite coffee beans or ground coffee. If you're using whole beans, grind them to a medium-coarse grind.\\n2. **Measure the coffee**: Use 1 tablespoon of coffee for every 6 ounces of water. For a standard cup, use about 10-12 grams of coffee.\\n3. **Place the filter**: Put a paper filter in the pour-over dripper.\\n4. **Add coffee**: Pour the measured coffee grounds into the filter.\\n5. **Add water**: Pour hot water (around 200째F) over the coffee grounds in a circular motion.\\n6. **Wait and pour**: Allow the coffee to drip through the filter (usually around 3-4 minutes). Once it's done, pour the coffee into your cup and enjoy!\\n\\nThese are just a few common methods for making a cup of coffee. Feel free to experiment with different techniques, coffee beans, and ratios to find your perfect cup!\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:09:16,922 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4543 - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "2025-06-05 14:09:16,923 - DEBUG - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1371 - Logging Details LiteLLM-Success Call streaming complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 14:09:16,925 - DEBUG - Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m14:09:16 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:09:16,926 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:09:16,927 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 1.87e-06, completion_tokens_cost_usd_dollar: 0.00027064\n",
      "2025-06-05 14:09:16,928 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 1.87e-06, completion_tokens_cost_usd_dollar: 0.00027064\n",
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.00027251\n",
      "2025-06-05 14:09:16,928 - DEBUG - response_cost: 0.00027251\n",
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:09:16,929 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:09:16 - LiteLLM:DEBUG\u001b[0m: utils.py:4543 - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "2025-06-05 14:09:16,930 - DEBUG - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1346 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "2025-06-05 14:13:29,303 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m14:13:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:13:29,305 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:29,306 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 8.69e-05, completion_tokens_cost_usd_dollar: 9.214e-05\n",
      "2025-06-05 14:13:29,307 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 8.69e-05, completion_tokens_cost_usd_dollar: 9.214e-05\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.00017904\n",
      "2025-06-05 14:13:29,309 - DEBUG - response_cost: 0.00017904\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:29,310 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:4543 - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "2025-06-05 14:13:29,310 - DEBUG - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1371 - Logging Details LiteLLM-Success Call streaming complete\n",
      "2025-06-05 14:13:29,312 - DEBUG - Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m14:13:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:13:29,313 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:29,315 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 8.69e-05, completion_tokens_cost_usd_dollar: 9.214e-05\n",
      "2025-06-05 14:13:29,316 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 8.69e-05, completion_tokens_cost_usd_dollar: 9.214e-05\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.00017904\n",
      "2025-06-05 14:13:29,317 - DEBUG - response_cost: 0.00017904\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:29,318 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:4543 - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "2025-06-05 14:13:29,319 - DEBUG - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1346 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "2025-06-05 14:13:30,245 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m14:13:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:13:30,247 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:30,248 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 2.8380000000000003e-05, completion_tokens_cost_usd_dollar: 7.955999999999999e-05\n",
      "2025-06-05 14:13:30,249 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 2.8380000000000003e-05, completion_tokens_cost_usd_dollar: 7.955999999999999e-05\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.00010794\n",
      "2025-06-05 14:13:30,250 - DEBUG - response_cost: 0.00010794\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:30,251 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4543 - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "2025-06-05 14:13:30,252 - DEBUG - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1371 - Logging Details LiteLLM-Success Call streaming complete\n",
      "2025-06-05 14:13:30,254 - DEBUG - Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m14:13:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:13:30,255 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:30,256 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 2.8380000000000003e-05, completion_tokens_cost_usd_dollar: 7.955999999999999e-05\n",
      "2025-06-05 14:13:30,257 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 2.8380000000000003e-05, completion_tokens_cost_usd_dollar: 7.955999999999999e-05\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.00010794\n",
      "2025-06-05 14:13:30,258 - DEBUG - response_cost: 0.00010794\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:30,259 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4543 - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "2025-06-05 14:13:30,260 - DEBUG - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1346 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "2025-06-05 14:13:31,475 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m14:13:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:13:31,476 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:31,478 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.809000000000001e-05, completion_tokens_cost_usd_dollar: 0.00011661999999999999\n",
      "2025-06-05 14:13:31,479 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.809000000000001e-05, completion_tokens_cost_usd_dollar: 0.00011661999999999999\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.00018470999999999998\n",
      "2025-06-05 14:13:31,481 - DEBUG - response_cost: 0.00018470999999999998\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:31,482 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4543 - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "2025-06-05 14:13:31,483 - DEBUG - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1371 - Logging Details LiteLLM-Success Call streaming complete\n",
      "2025-06-05 14:13:31,484 - DEBUG - Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m14:13:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:13:31,485 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:31,486 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.809000000000001e-05, completion_tokens_cost_usd_dollar: 0.00011661999999999999\n",
      "2025-06-05 14:13:31,487 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.809000000000001e-05, completion_tokens_cost_usd_dollar: 0.00011661999999999999\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.00018470999999999998\n",
      "2025-06-05 14:13:31,488 - DEBUG - response_cost: 0.00018470999999999998\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:31,490 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4543 - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "2025-06-05 14:13:31,491 - DEBUG - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1346 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "2025-06-05 14:13:34,142 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m14:13:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:13:34,144 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:34,146 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.556e-05, completion_tokens_cost_usd_dollar: 0.00033116\n",
      "2025-06-05 14:13:34,148 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.556e-05, completion_tokens_cost_usd_dollar: 0.00033116\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.00039672\n",
      "2025-06-05 14:13:34,150 - DEBUG - response_cost: 0.00039672\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:34,151 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4543 - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "2025-06-05 14:13:34,153 - DEBUG - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1371 - Logging Details LiteLLM-Success Call streaming complete\n",
      "2025-06-05 14:13:34,155 - DEBUG - Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m14:13:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:13:34,157 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:34,158 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.556e-05, completion_tokens_cost_usd_dollar: 0.00033116\n",
      "2025-06-05 14:13:34,160 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.556e-05, completion_tokens_cost_usd_dollar: 0.00033116\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.00039672\n",
      "2025-06-05 14:13:34,161 - DEBUG - response_cost: 0.00039672\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:34,163 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4543 - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "2025-06-05 14:13:34,165 - DEBUG - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1346 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "2025-06-05 14:13:35,945 - DEBUG - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
      "\u001b[92m14:13:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:13:35,946 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:35,947 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.578e-05, completion_tokens_cost_usd_dollar: 0.00021522\n",
      "2025-06-05 14:13:35,949 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.578e-05, completion_tokens_cost_usd_dollar: 0.00021522\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.000281\n",
      "2025-06-05 14:13:35,950 - DEBUG - response_cost: 0.000281\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:35,951 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4543 - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "2025-06-05 14:13:35,951 - DEBUG - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1371 - Logging Details LiteLLM-Success Call streaming complete\n",
      "2025-06-05 14:13:35,952 - DEBUG - Logging Details LiteLLM-Success Call streaming complete\n",
      "\u001b[92m14:13:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:13:35,954 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:35,955 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.578e-05, completion_tokens_cost_usd_dollar: 0.00021522\n",
      "2025-06-05 14:13:35,956 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.578e-05, completion_tokens_cost_usd_dollar: 0.00021522\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.000281\n",
      "2025-06-05 14:13:35,957 - DEBUG - response_cost: 0.000281\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:35,958 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4543 - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n",
      "2025-06-05 14:13:35,959 - DEBUG - model_info: {'key': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'max_tokens': 8192, 'max_input_tokens': 131072, 'max_output_tokens': 8192, 'input_cost_per_token': 1.1e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 3.4e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_reasoning_token': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"how to make a cup of coffee\"\n",
    "llama_model.call([{\"role\": \"user\", \"content\": prompt}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1b15221",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Resume Parsing Agent\n",
    "class ResumeParsingAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            role=\"Resume Parser\",\n",
    "            backstory=\"I extract structured data from resumes with high accuracy.\",\n",
    "            goal=\"Parse resumes to extract skills, experience, education, certifications, and career gaps.\"\n",
    "        )\n",
    "\n",
    "    def execute_task(self, task: Task, context: list = None, tools: list = None):\n",
    "        resume_content = task.description\n",
    "        if not resume_content:\n",
    "            raise ValueError(\"Resume content is required.\")\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        You are a resume parser. Extract the following fields from the text:\n",
    "        - Name\n",
    "        - Education\n",
    "        - Skills\n",
    "        - Work experience (roles, companies, duration)\n",
    "\n",
    "        Resume:\n",
    "        {resume_content}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            structured_data = self.llm.call([{\"role\": \"user\", \"content\": prompt}])\n",
    "            \n",
    "            #parsed_data = safe_parse_json(structured_data, {\"error\": \"Failed to parse resume\", \"raw\": structured_data})\n",
    "            if \"error\" in structured_data:\n",
    "                raise ValueError(f\"Resume parsing failed: {structured_data['error']}\")\n",
    "            return json.dumps({ \"resume\": structured_data.strip() }) # Return parsed JSON directly\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error parsing resume: {str(e)}\")\n",
    "\n",
    "# JD Understanding Agent\n",
    "class JDUnderstandingAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            role=\"JD Parser\",\n",
    "            backstory=\"I analyze job descriptions to extract key requirements.\",\n",
    "            goal=\"Extract mandatory/optional skills, seniority, and soft skills from job descriptions.\"\n",
    "        )\n",
    "\n",
    "    def execute_task(self, task: Task, context: list = None, tools: list = None):\n",
    "        jd_content = task.description\n",
    "        if not jd_content:\n",
    "            raise ValueError(\"JD content is required.\")\n",
    "        \n",
    "        prompt =   f\"\"\"\n",
    "        You are a JD parser. Extract:\n",
    "        - Job title\n",
    "        - Responsibilities\n",
    "        - Required/Preferred skills\n",
    "\n",
    "        Job Description:\n",
    "        {jd_content}\n",
    "        \"\"\"\n",
    "        try:\n",
    "            structured_data = self.llm.call([{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "            return json.dumps({ \"job_description\": structured_data.strip() })  # Return parsed JSON directly\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error parsing JD: {str(e)}\")\n",
    "\n",
    "# Candidate-Role Matching Agent\n",
    "class MatchingAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            role=\"Candidate-Role Matcher\",\n",
    "            backstory=\"I match candidates to roles based on skills and experience.\",\n",
    "            goal=\"Compute a match score between resume and job description.\"\n",
    "        )\n",
    "\n",
    "    def execute_task(self, task: Task, context: dict = None, tools: list = None):\n",
    "        # Debug: Log context type and content\n",
    "        print(f\"Debug - MatchingAgent context type: {type(context)}\")\n",
    "        print(f\"Debug - MatchingAgent context content: {context}\")\n",
    "        #context = json.loads(context)\n",
    "        #contexts = context.split(\"----------\")\n",
    "\n",
    "        #resume_data, jd_data = context.split(\"----------\")\n",
    "        #if not resume_data or not jd_data:\n",
    "        #    raise ValueError(f\"Empty data received. Resume: {resume_data}, JD: {jd_data}\")\n",
    "        \n",
    "        # Ensure inputs are JSON objects\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        Compare the following resume and job_description info.\n",
    "        List skills or experiences that MATCH and those that are MISSING(present in Job Description but not in Resume ).\n",
    "        Respond as : {{\"matched_skills\": [], \"missing_skills\": [], \"match_score\": 0.0}}\n",
    "        context: {context}        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            match_result = self.llm.call([{\"role\": \"user\", \"content\": prompt}])\n",
    "\n",
    "            return match_result  # Return parsed JSON\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error matching resume and JD: {str(e)}\")\n",
    "\n",
    "\n",
    "class ResumeEnhancerAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            role=\"Resume Enhancer\",\n",
    "            backstory=\"I suggest improvements to the resume to make it more relevant to the JD.\",\n",
    "            goal=\"Optimize resumes based on job descriptions.\"\n",
    "        )\n",
    "\n",
    "    def execute_task(self, task: Task, context: dict = None, tools: list = None):\n",
    "        prompt = f\"\"\"\n",
    "        Given this resume and job description, suggest improvements to the resume:\n",
    "        - Highlight missing skills\n",
    "        - Recommend better phrasing\n",
    "        - Suggest added sections or content\n",
    "\n",
    "        context: {context}\n",
    "        \"\"\"\n",
    "        result = self.llm.call([{\"role\": \"user\", \"content\": prompt}])\n",
    "        return json.dumps({\"resume_enhancement\": result.strip()})\n",
    "\n",
    "\n",
    "class CoverLetterAgent(Agent):\n",
    "    def __init__(self, llm):\n",
    "        super().__init__(\n",
    "            llm=llm,\n",
    "            role=\"Cover Letter Generator\",\n",
    "            backstory=\"I write cover letters tailored to job descriptions and candidate profiles.\",\n",
    "            goal=\"Generate persuasive cover letters that address gaps and emphasize fit.\"\n",
    "        )\n",
    "\n",
    "    def execute_task(self, task: Task, context: dict = None, tools: list = None):\n",
    "        prompt = f\"\"\"\n",
    "        Generate a professional cover letter that:\n",
    "        - Highlights the candidate's strengths\n",
    "        - Acknowledges and bridges skill or experience gaps\n",
    "        - Aligns with the job description and tone\n",
    "\n",
    "        context: {context}\n",
    "        \"\"\"\n",
    "        result = self.llm.call([{\"role\": \"user\", \"content\": prompt}])\n",
    "        return json.dumps({\"cover_letter\": result.strip()})\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b2182b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_content = \"\"\"1  Vikram Bhat  Dublin, Ireland  +353-894410686  Vikrambhat249@gmail.com   Experienced Data Scientist with 8+ years of experience specializing in large language models (LLMs), Retrieval-Augmented Generation (RAG) systems, and scalable AI infrastructure. Proven track record of building robust pipelines for RAG systems, output validation, and hallucination mitigation. Strong foundation in mathematics, statistics, and software engineering, with deep experience in deploying production-grade ML systems. Passionate about open-source innovation and advancing responsible AI practices that align with human values. Work Experience:  IBM, Dublin Data Scientist May 2019  Present  Designed and deployed Retrieval-Augmented Generation (RAG) systems using Watsonx LLMs to enhance search accuracy and reduce query latency.  Integrated LangChain with vector stores such as Milvus and Elasticsearch to enable efficient dense retrieval in production NLP pipelines.  Developed machine learning pipelines on AWS SageMaker and IBM Watson Studio for model training, deployment, and post-deployment monitoring.  Built content indexing and retrieval infrastructure to optimize document ingestion and search efficiency.  Delivered predictive models for clients in healthcare and utilities by applying advanced feature engineering and model optimization techniques.  Automated processes to monitor, detect, and resolve data anomalies, ensuring consistent data quality and integrity.  Provided hands-on mentorship within the team.  Created interactive streamlit and R Shiny dashboards to support operational decision-making and improve stakeholder visibility. Voxpro Groups, Cork Data Analyst Jan 2018  Apr 2019  Streamlined data integration by extracting and transforming structured and unstructured data from diverse internal tools, improving data accuracy and accessibility.  Enhanced business insights by building predictive models, generating comprehensive reports, and creating visualization solutions using Python and Power BI.  Delivered actionable insights, enabling data-driven decision-making and improving operational efficiency. 2 Cognizant Technology Solutions, Bengaluru BI Developer Jul 2012  Aug 2016  Optimized ETL operations for diverse data sources, including Mainframes and SQL Servers, improving data processing efficiency and integration.  Developed and implemented robust data mappings in Hadoop environments, improving data preparation and ensuring quality for downstream analytics.  Collaborated with cross-functional teams to design scalable BI solutions, enhancing data accessibility and analysis. AI Application Development  Developed and deployed several AI applications using CrewAI, LangChain, and Llama models, including conversational chatbots, RAG pipelines, and fact-checking tools.  Authored technical blog posts on building AI applications, NLP projects, sharing insights and best practices. Visit my Medium blog for detailed write-ups: Medium Blog. Skills:  Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)  Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines  Programming: Python, R, SQL  Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash  Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers  Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines Education:  MSc in Data Science and Analytics, University College Cork, Cork Graduated: October 2017 | GPA: 1:1  Scholarship: Boole / Presidential Merit Scholarships for top international student 2016/2017  Project: Maximized students experience in a university using Artificial Neural Networks and Support Vector Machines B.E. in Computer Science, Sir M.VIT, Bengaluru Graduated: June 2012  Coursework: Relational Databases, Compiler Design, Data Structures, Advanced Mathematics and Statistics\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ef88d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "jd_content = \"\"\"Primary Responsibilities    Analyze, review and interpret complex data through the development of case-studies, enabling the data to tell a story  Build and maintain production-ready machine-learning models  Identify drivers of patterns / behaviors uncovered in data and use these to explain business trends  Contribute to the design and realization of analytical based tools/assets to identify and monitor non-standard claim patterns  Engage with subject-matter experts to explore business problems and design solutions  Present analysis and interpretation for both operational and business review and planning  Support short and long term operational and strategic business activities through the use of data and develop recommended business solutions through research, analysis and implement when appropriate   You will be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role, as well as providing development for other roles you may be interested in.  Required Qualifications    Master's degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain with a significant quantitative aspect  Proficiency in AI/ML and deep learning frameworks such us PyTorch  Proven track record of developing and deploying machine learning models in production environments\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fb21e2d",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "resume_parser = ResumeParsingAgent(llm=llama_model)\n",
    "jd_parser = JDUnderstandingAgent(llm=llama_model)\n",
    "matcher = MatchingAgent(llm=llama_model)\n",
    "resumeenhancer = ResumeEnhancerAgent(llm=llama_model)\n",
    "cv=CoverLetterAgent(llm=llama_model)\n",
    "\n",
    "# Task definitions\n",
    "resume_task = Task(\n",
    "    description=resume_content,\n",
    "    expected_output=\"Structured JSON with skills, experience, education, certifications, and career gaps.\",\n",
    "    agent=resume_parser\n",
    ")\n",
    "\n",
    "jd_task = Task(\n",
    "    description=jd_content,\n",
    "    expected_output=\"Structured JSON with mandatory/optional skills, seniority, and soft skills.\",\n",
    "    agent=jd_parser\n",
    ")\n",
    "matching_task = Task(\n",
    "        description=\"Match resume to JD.\",\n",
    "        expected_output=\"JSON with match score, skill matches, experience match, and gaps.\",\n",
    "        agent=matcher,\n",
    "        context=[resume_task, jd_task]\n",
    "    )\n",
    "\n",
    "resumeenhancer_task = Task(\n",
    "        description=\"Optimize resumes based on job descriptions.\",\n",
    "        expected_output=\"Readable text report with improvements to the resume to make it more relevant to the JD.\",\n",
    "        agent=resumeenhancer,\n",
    "        context=[resume_task, jd_task]\n",
    "    )\n",
    "\n",
    "cv_task = Task(\n",
    "        description=\"Generate a cover letter based on the resume and job description.\",\n",
    "        expected_output=\"Readable text cover letter tailored to the job description.\",\n",
    "        agent=cv,\n",
    "        context=[resume_task, jd_task]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d44ab3",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** Machine Learning Analyst\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review, and interpret complex data through case-studies.\\\\n2. Build and maintain production-ready machine-learning models.\\\\n3. Identify drivers of patterns/behaviors in data to explain business trends.\\\\n4. Contribute to designing and realizing analytical tools/assets for identifying non-standard claim patterns.\\\\n5. Engage with subject-matter experts to explore business problems and design solutions.\\\\n6. Present analysis and interpretation for operational and business review and planning.\\\\n7. Support short-term and long-term operational and strategic business activities through data usage.\\\\n\\\\n**Required Skills:**\\\\n\\\\n1. Master\\'s degree in Statistics, Physics, Mathematics, Engineering (or similar analytic domain).\\\\n2. Proficiency in AI/ML frameworks like PyTorch.\\\\n3. Proven track record of developing and deploying machine learning models in production environments.\\\\n\\\\n**Preferred Skills:** None mentioned explicitly.\"}'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eda6e1",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 14:13:30,741 - DEBUG - Starting new HTTPS connection (1): telemetry.crewai.com:4319\n",
      "2025-06-05 14:13:31,213 - DEBUG - https://telemetry.crewai.com:4319 \"POST /v1/traces HTTP/1.1\" 200 2\n",
      "2025-06-05 14:13:35,905 - DEBUG - https://telemetry.crewai.com:4319 \"POST /v1/traces HTTP/1.1\" 200 2\n"
     ]
    }
   ],
   "source": [
    "crew = Crew(\n",
    "        agents=[resume_parser, jd_parser, matcher, resumeenhancer, cv],\n",
    "        tasks=[resume_task, jd_task, matching_task, resumeenhancer_task, cv_task],\n",
    "        name=\"Resume and JD Matching Crew\",\n",
    "        description=\"A crew to parse resumes, understand job descriptions, match them, and summarize the results.\",\n",
    "        verbose=True,\n",
    "        process=Process.sequential\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff65e534",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m14:13:27 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \n",
      "\n",
      "2025-06-05 14:13:27,833 - DEBUG - \n",
      "\n",
      "\u001b[92m14:13:27 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "2025-06-05 14:13:27,834 - DEBUG - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m14:13:27 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \u001b[92mlitellm.completion(model='groq/meta-llama/llama-4-scout-17b-16e-instruct', messages=[{'role': 'user', 'content': '\\n        You are a resume parser. Extract the following fields from the text:\\n        - Name\\n        - Education\\n        - Skills\\n        - Work experience (roles, companies, duration)\\n\\n        Resume:\\n        1  Vikram Bhat  Dublin, Ireland  +353-894410686  Vikrambhat249@gmail.com   Experienced Data Scientist with 8+ years of experience specializing in large language models (LLMs), Retrieval-Augmented Generation (RAG) systems, and scalable AI infrastructure. Proven track record of building robust pipelines for RAG systems, output validation, and hallucination mitigation. Strong foundation in mathematics, statistics, and software engineering, with deep experience in deploying production-grade ML systems. Passionate about open-source innovation and advancing responsible AI practices that align with human values. Work Experience:  IBM, Dublin Data Scientist May 2019  Present  Designed and deployed Retrieval-Augmented Generation (RAG) systems using Watsonx LLMs to enhance search accuracy and reduce query latency.  Integrated LangChain with vector stores such as Milvus and Elasticsearch to enable efficient dense retrieval in production NLP pipelines.  Developed machine learning pipelines on AWS SageMaker and IBM Watson Studio for model training, deployment, and post-deployment monitoring.  Built content indexing and retrieval infrastructure to optimize document ingestion and search efficiency.  Delivered predictive models for clients in healthcare and utilities by applying advanced feature engineering and model optimization techniques.  Automated processes to monitor, detect, and resolve data anomalies, ensuring consistent data quality and integrity.  Provided hands-on mentorship within the team.  Created interactive streamlit and R Shiny dashboards to support operational decision-making and improve stakeholder visibility. Voxpro Groups, Cork Data Analyst Jan 2018  Apr 2019  Streamlined data integration by extracting and transforming structured and unstructured data from diverse internal tools, improving data accuracy and accessibility.  Enhanced business insights by building predictive models, generating comprehensive reports, and creating visualization solutions using Python and Power BI.  Delivered actionable insights, enabling data-driven decision-making and improving operational efficiency. 2 Cognizant Technology Solutions, Bengaluru BI Developer Jul 2012  Aug 2016  Optimized ETL operations for diverse data sources, including Mainframes and SQL Servers, improving data processing efficiency and integration.  Developed and implemented robust data mappings in Hadoop environments, improving data preparation and ensuring quality for downstream analytics.  Collaborated with cross-functional teams to design scalable BI solutions, enhancing data accessibility and analysis. AI Application Development  Developed and deployed several AI applications using CrewAI, LangChain, and Llama models, including conversational chatbots, RAG pipelines, and fact-checking tools.  Authored technical blog posts on building AI applications, NLP projects, sharing insights and best practices. Visit my Medium blog for detailed write-ups: Medium Blog. Skills:  Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)  Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines  Programming: Python, R, SQL  Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash  Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers  Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines Education:  MSc in Data Science and Analytics, University College Cork, Cork Graduated: October 2017 | GPA: 1:1  Scholarship: Boole / Presidential Merit Scholarships for top international student 2016/2017  Project: Maximized students experience in a university using Artificial Neural Networks and Support Vector Machines B.E. in Computer Science, Sir M.VIT, Bengaluru Graduated: June 2012  Coursework: Relational Databases, Compiler Design, Data Structures, Advanced Mathematics and Statistics\\n        '}], temperature=0.7, stop=['\\nObservation:'], stream=False)\u001b[0m\n",
      "2025-06-05 14:13:27,835 - DEBUG - \u001b[92mlitellm.completion(model='groq/meta-llama/llama-4-scout-17b-16e-instruct', messages=[{'role': 'user', 'content': '\\n        You are a resume parser. Extract the following fields from the text:\\n        - Name\\n        - Education\\n        - Skills\\n        - Work experience (roles, companies, duration)\\n\\n        Resume:\\n        1  Vikram Bhat  Dublin, Ireland  +353-894410686  Vikrambhat249@gmail.com   Experienced Data Scientist with 8+ years of experience specializing in large language models (LLMs), Retrieval-Augmented Generation (RAG) systems, and scalable AI infrastructure. Proven track record of building robust pipelines for RAG systems, output validation, and hallucination mitigation. Strong foundation in mathematics, statistics, and software engineering, with deep experience in deploying production-grade ML systems. Passionate about open-source innovation and advancing responsible AI practices that align with human values. Work Experience:  IBM, Dublin Data Scientist May 2019  Present  Designed and deployed Retrieval-Augmented Generation (RAG) systems using Watsonx LLMs to enhance search accuracy and reduce query latency.  Integrated LangChain with vector stores such as Milvus and Elasticsearch to enable efficient dense retrieval in production NLP pipelines.  Developed machine learning pipelines on AWS SageMaker and IBM Watson Studio for model training, deployment, and post-deployment monitoring.  Built content indexing and retrieval infrastructure to optimize document ingestion and search efficiency.  Delivered predictive models for clients in healthcare and utilities by applying advanced feature engineering and model optimization techniques.  Automated processes to monitor, detect, and resolve data anomalies, ensuring consistent data quality and integrity.  Provided hands-on mentorship within the team.  Created interactive streamlit and R Shiny dashboards to support operational decision-making and improve stakeholder visibility. Voxpro Groups, Cork Data Analyst Jan 2018  Apr 2019  Streamlined data integration by extracting and transforming structured and unstructured data from diverse internal tools, improving data accuracy and accessibility.  Enhanced business insights by building predictive models, generating comprehensive reports, and creating visualization solutions using Python and Power BI.  Delivered actionable insights, enabling data-driven decision-making and improving operational efficiency. 2 Cognizant Technology Solutions, Bengaluru BI Developer Jul 2012  Aug 2016  Optimized ETL operations for diverse data sources, including Mainframes and SQL Servers, improving data processing efficiency and integration.  Developed and implemented robust data mappings in Hadoop environments, improving data preparation and ensuring quality for downstream analytics.  Collaborated with cross-functional teams to design scalable BI solutions, enhancing data accessibility and analysis. AI Application Development  Developed and deployed several AI applications using CrewAI, LangChain, and Llama models, including conversational chatbots, RAG pipelines, and fact-checking tools.  Authored technical blog posts on building AI applications, NLP projects, sharing insights and best practices. Visit my Medium blog for detailed write-ups: Medium Blog. Skills:  Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)  Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines  Programming: Python, R, SQL  Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash  Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers  Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines Education:  MSc in Data Science and Analytics, University College Cork, Cork Graduated: October 2017 | GPA: 1:1  Scholarship: Boole / Presidential Merit Scholarships for top international student 2016/2017  Project: Maximized students experience in a university using Artificial Neural Networks and Support Vector Machines B.E. in Computer Science, Sir M.VIT, Bengaluru Graduated: June 2012  Coursework: Relational Databases, Compiler Design, Data Structures, Advanced Mathematics and Statistics\\n        '}], temperature=0.7, stop=['\\nObservation:'], stream=False)\u001b[0m\n",
      "\u001b[92m14:13:27 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \n",
      "\n",
      "2025-06-05 14:13:27,835 - DEBUG - \n",
      "\n",
      "\u001b[92m14:13:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:458 - self.optional_params: {}\n",
      "2025-06-05 14:13:27,836 - DEBUG - self.optional_params: {}\n",
      "\u001b[92m14:13:27 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "2025-06-05 14:13:27,836 - DEBUG - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m14:13:27 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-scout-17b-16e-instruct; provider = groq\n",
      "2025-06-05 14:13:27,837 - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-scout-17b-16e-instruct; provider = groq\n",
      "\u001b[92m14:13:27 - LiteLLM:DEBUG\u001b[0m: utils.py:2908 - \n",
      "LiteLLM: Params passed to completion() {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'functions': None, 'function_call': None, 'temperature': 0.7, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': ['\\nObservation:'], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': '\\n        You are a resume parser. Extract the following fields from the text:\\n        - Name\\n        - Education\\n        - Skills\\n        - Work experience (roles, companies, duration)\\n\\n        Resume:\\n        1  Vikram Bhat  Dublin, Ireland  +353-894410686  Vikrambhat249@gmail.com   Experienced Data Scientist with 8+ years of experience specializing in large language models (LLMs), Retrieval-Augmented Generation (RAG) systems, and scalable AI infrastructure. Proven track record of building robust pipelines for RAG systems, output validation, and hallucination mitigation. Strong foundation in mathematics, statistics, and software engineering, with deep experience in deploying production-grade ML systems. Passionate about open-source innovation and advancing responsible AI practices that align with human values. Work Experience:  IBM, Dublin Data Scientist May 2019  Present  Designed and deployed Retrieval-Augmented Generation (RAG) systems using Watsonx LLMs to enhance search accuracy and reduce query latency.  Integrated LangChain with vector stores such as Milvus and Elasticsearch to enable efficient dense retrieval in production NLP pipelines.  Developed machine learning pipelines on AWS SageMaker and IBM Watson Studio for model training, deployment, and post-deployment monitoring.  Built content indexing and retrieval infrastructure to optimize document ingestion and search efficiency.  Delivered predictive models for clients in healthcare and utilities by applying advanced feature engineering and model optimization techniques.  Automated processes to monitor, detect, and resolve data anomalies, ensuring consistent data quality and integrity.  Provided hands-on mentorship within the team.  Created interactive streamlit and R Shiny dashboards to support operational decision-making and improve stakeholder visibility. Voxpro Groups, Cork Data Analyst Jan 2018  Apr 2019  Streamlined data integration by extracting and transforming structured and unstructured data from diverse internal tools, improving data accuracy and accessibility.  Enhanced business insights by building predictive models, generating comprehensive reports, and creating visualization solutions using Python and Power BI.  Delivered actionable insights, enabling data-driven decision-making and improving operational efficiency. 2 Cognizant Technology Solutions, Bengaluru BI Developer Jul 2012  Aug 2016  Optimized ETL operations for diverse data sources, including Mainframes and SQL Servers, improving data processing efficiency and integration.  Developed and implemented robust data mappings in Hadoop environments, improving data preparation and ensuring quality for downstream analytics.  Collaborated with cross-functional teams to design scalable BI solutions, enhancing data accessibility and analysis. AI Application Development  Developed and deployed several AI applications using CrewAI, LangChain, and Llama models, including conversational chatbots, RAG pipelines, and fact-checking tools.  Authored technical blog posts on building AI applications, NLP projects, sharing insights and best practices. Visit my Medium blog for detailed write-ups: Medium Blog. Skills:  Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)  Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines  Programming: Python, R, SQL  Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash  Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers  Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines Education:  MSc in Data Science and Analytics, University College Cork, Cork Graduated: October 2017 | GPA: 1:1  Scholarship: Boole / Presidential Merit Scholarships for top international student 2016/2017  Project: Maximized students experience in a university using Artificial Neural Networks and Support Vector Machines B.E. in Computer Science, Sir M.VIT, Bengaluru Graduated: June 2012  Coursework: Relational Databases, Compiler Design, Data Structures, Advanced Mathematics and Statistics\\n        '}], 'thinking': None, 'web_search_options': None}\n",
      "2025-06-05 14:13:27,837 - DEBUG - \n",
      "LiteLLM: Params passed to completion() {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'functions': None, 'function_call': None, 'temperature': 0.7, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': ['\\nObservation:'], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': '\\n        You are a resume parser. Extract the following fields from the text:\\n        - Name\\n        - Education\\n        - Skills\\n        - Work experience (roles, companies, duration)\\n\\n        Resume:\\n        1  Vikram Bhat  Dublin, Ireland  +353-894410686  Vikrambhat249@gmail.com   Experienced Data Scientist with 8+ years of experience specializing in large language models (LLMs), Retrieval-Augmented Generation (RAG) systems, and scalable AI infrastructure. Proven track record of building robust pipelines for RAG systems, output validation, and hallucination mitigation. Strong foundation in mathematics, statistics, and software engineering, with deep experience in deploying production-grade ML systems. Passionate about open-source innovation and advancing responsible AI practices that align with human values. Work Experience:  IBM, Dublin Data Scientist May 2019  Present  Designed and deployed Retrieval-Augmented Generation (RAG) systems using Watsonx LLMs to enhance search accuracy and reduce query latency.  Integrated LangChain with vector stores such as Milvus and Elasticsearch to enable efficient dense retrieval in production NLP pipelines.  Developed machine learning pipelines on AWS SageMaker and IBM Watson Studio for model training, deployment, and post-deployment monitoring.  Built content indexing and retrieval infrastructure to optimize document ingestion and search efficiency.  Delivered predictive models for clients in healthcare and utilities by applying advanced feature engineering and model optimization techniques.  Automated processes to monitor, detect, and resolve data anomalies, ensuring consistent data quality and integrity.  Provided hands-on mentorship within the team.  Created interactive streamlit and R Shiny dashboards to support operational decision-making and improve stakeholder visibility. Voxpro Groups, Cork Data Analyst Jan 2018  Apr 2019  Streamlined data integration by extracting and transforming structured and unstructured data from diverse internal tools, improving data accuracy and accessibility.  Enhanced business insights by building predictive models, generating comprehensive reports, and creating visualization solutions using Python and Power BI.  Delivered actionable insights, enabling data-driven decision-making and improving operational efficiency. 2 Cognizant Technology Solutions, Bengaluru BI Developer Jul 2012  Aug 2016  Optimized ETL operations for diverse data sources, including Mainframes and SQL Servers, improving data processing efficiency and integration.  Developed and implemented robust data mappings in Hadoop environments, improving data preparation and ensuring quality for downstream analytics.  Collaborated with cross-functional teams to design scalable BI solutions, enhancing data accessibility and analysis. AI Application Development  Developed and deployed several AI applications using CrewAI, LangChain, and Llama models, including conversational chatbots, RAG pipelines, and fact-checking tools.  Authored technical blog posts on building AI applications, NLP projects, sharing insights and best practices. Visit my Medium blog for detailed write-ups: Medium Blog. Skills:  Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)  Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines  Programming: Python, R, SQL  Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash  Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers  Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines Education:  MSc in Data Science and Analytics, University College Cork, Cork Graduated: October 2017 | GPA: 1:1  Scholarship: Boole / Presidential Merit Scholarships for top international student 2016/2017  Project: Maximized students experience in a university using Artificial Neural Networks and Support Vector Machines B.E. in Computer Science, Sir M.VIT, Bengaluru Graduated: June 2012  Coursework: Relational Databases, Compiler Design, Data Structures, Advanced Mathematics and Statistics\\n        '}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m14:13:27 - LiteLLM:DEBUG\u001b[0m: utils.py:2911 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}\n",
      "2025-06-05 14:13:27,838 - DEBUG - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}\n",
      "\u001b[92m14:13:27 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - Final returned optional params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "2025-06-05 14:13:27,838 - DEBUG - Final returned optional params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "\u001b[92m14:13:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:458 - self.optional_params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "2025-06-05 14:13:27,839 - DEBUG - self.optional_params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "\u001b[92m14:13:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:901 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.groq.com/openai/v1/chat/completions \\\n",
      "-H 'Authorization: Be****An' -H 'Content-Type: ap****on' \\\n",
      "-d '{'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'messages': [{'role': 'user', 'content': '\\n        You are a resume parser. Extract the following fields from the text:\\n        - Name\\n        - Education\\n        - Skills\\n        - Work experience (roles, companies, duration)\\n\\n        Resume:\\n        1  Vikram Bhat  Dublin, Ireland  +353-894410686  Vikrambhat249@gmail.com   Experienced Data Scientist with 8+ years of experience specializing in large language models (LLMs), Retrieval-Augmented Generation (RAG) systems, and scalable AI infrastructure. Proven track record of building robust pipelines for RAG systems, output validation, and hallucination mitigation. Strong foundation in mathematics, statistics, and software engineering, with deep experience in deploying production-grade ML systems. Passionate about open-source innovation and advancing responsible AI practices that align with human values. Work Experience:  IBM, Dublin Data Scientist May 2019  Present  Designed and deployed Retrieval-Augmented Generation (RAG) systems using Watsonx LLMs to enhance search accuracy and reduce query latency.  Integrated LangChain with vector stores such as Milvus and Elasticsearch to enable efficient dense retrieval in production NLP pipelines.  Developed machine learning pipelines on AWS SageMaker and IBM Watson Studio for model training, deployment, and post-deployment monitoring.  Built content indexing and retrieval infrastructure to optimize document ingestion and search efficiency.  Delivered predictive models for clients in healthcare and utilities by applying advanced feature engineering and model optimization techniques.  Automated processes to monitor, detect, and resolve data anomalies, ensuring consistent data quality and integrity.  Provided hands-on mentorship within the team.  Created interactive streamlit and R Shiny dashboards to support operational decision-making and improve stakeholder visibility. Voxpro Groups, Cork Data Analyst Jan 2018  Apr 2019  Streamlined data integration by extracting and transforming structured and unstructured data from diverse internal tools, improving data accuracy and accessibility.  Enhanced business insights by building predictive models, generating comprehensive reports, and creating visualization solutions using Python and Power BI.  Delivered actionable insights, enabling data-driven decision-making and improving operational efficiency. 2 Cognizant Technology Solutions, Bengaluru BI Developer Jul 2012  Aug 2016  Optimized ETL operations for diverse data sources, including Mainframes and SQL Servers, improving data processing efficiency and integration.  Developed and implemented robust data mappings in Hadoop environments, improving data preparation and ensuring quality for downstream analytics.  Collaborated with cross-functional teams to design scalable BI solutions, enhancing data accessibility and analysis. AI Application Development  Developed and deployed several AI applications using CrewAI, LangChain, and Llama models, including conversational chatbots, RAG pipelines, and fact-checking tools.  Authored technical blog posts on building AI applications, NLP projects, sharing insights and best practices. Visit my Medium blog for detailed write-ups: Medium Blog. Skills:  Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)  Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines  Programming: Python, R, SQL  Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash  Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers  Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines Education:  MSc in Data Science and Analytics, University College Cork, Cork Graduated: October 2017 | GPA: 1:1  Scholarship: Boole / Presidential Merit Scholarships for top international student 2016/2017  Project: Maximized students experience in a university using Artificial Neural Networks and Support Vector Machines B.E. in Computer Science, Sir M.VIT, Bengaluru Graduated: June 2012  Coursework: Relational Databases, Compiler Design, Data Structures, Advanced Mathematics and Statistics\\n        '}], 'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}'\n",
      "\u001b[0m\n",
      "\n",
      "2025-06-05 14:13:27,840 - DEBUG - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.groq.com/openai/v1/chat/completions \\\n",
      "-H 'Authorization: Be****An' -H 'Content-Type: ap****on' \\\n",
      "-d '{'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'messages': [{'role': 'user', 'content': '\\n        You are a resume parser. Extract the following fields from the text:\\n        - Name\\n        - Education\\n        - Skills\\n        - Work experience (roles, companies, duration)\\n\\n        Resume:\\n        1  Vikram Bhat  Dublin, Ireland  +353-894410686  Vikrambhat249@gmail.com   Experienced Data Scientist with 8+ years of experience specializing in large language models (LLMs), Retrieval-Augmented Generation (RAG) systems, and scalable AI infrastructure. Proven track record of building robust pipelines for RAG systems, output validation, and hallucination mitigation. Strong foundation in mathematics, statistics, and software engineering, with deep experience in deploying production-grade ML systems. Passionate about open-source innovation and advancing responsible AI practices that align with human values. Work Experience:  IBM, Dublin Data Scientist May 2019  Present  Designed and deployed Retrieval-Augmented Generation (RAG) systems using Watsonx LLMs to enhance search accuracy and reduce query latency.  Integrated LangChain with vector stores such as Milvus and Elasticsearch to enable efficient dense retrieval in production NLP pipelines.  Developed machine learning pipelines on AWS SageMaker and IBM Watson Studio for model training, deployment, and post-deployment monitoring.  Built content indexing and retrieval infrastructure to optimize document ingestion and search efficiency.  Delivered predictive models for clients in healthcare and utilities by applying advanced feature engineering and model optimization techniques.  Automated processes to monitor, detect, and resolve data anomalies, ensuring consistent data quality and integrity.  Provided hands-on mentorship within the team.  Created interactive streamlit and R Shiny dashboards to support operational decision-making and improve stakeholder visibility. Voxpro Groups, Cork Data Analyst Jan 2018  Apr 2019  Streamlined data integration by extracting and transforming structured and unstructured data from diverse internal tools, improving data accuracy and accessibility.  Enhanced business insights by building predictive models, generating comprehensive reports, and creating visualization solutions using Python and Power BI.  Delivered actionable insights, enabling data-driven decision-making and improving operational efficiency. 2 Cognizant Technology Solutions, Bengaluru BI Developer Jul 2012  Aug 2016  Optimized ETL operations for diverse data sources, including Mainframes and SQL Servers, improving data processing efficiency and integration.  Developed and implemented robust data mappings in Hadoop environments, improving data preparation and ensuring quality for downstream analytics.  Collaborated with cross-functional teams to design scalable BI solutions, enhancing data accessibility and analysis. AI Application Development  Developed and deployed several AI applications using CrewAI, LangChain, and Llama models, including conversational chatbots, RAG pipelines, and fact-checking tools.  Authored technical blog posts on building AI applications, NLP projects, sharing insights and best practices. Visit my Medium blog for detailed write-ups: Medium Blog. Skills:  Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)  Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines  Programming: Python, R, SQL  Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash  Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers  Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines Education:  MSc in Data Science and Analytics, University College Cork, Cork Graduated: October 2017 | GPA: 1:1  Scholarship: Boole / Presidential Merit Scholarships for top international student 2016/2017  Project: Maximized students experience in a university using Artificial Neural Networks and Support Vector Machines B.E. in Computer Science, Sir M.VIT, Bengaluru Graduated: June 2012  Coursework: Relational Databases, Compiler Design, Data Structures, Advanced Mathematics and Statistics\\n        '}], 'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}'\n",
      "\u001b[0m\n",
      "\n",
      "2025-06-05 14:13:27,841 - DEBUG - close.started\n",
      "2025-06-05 14:13:27,842 - DEBUG - close.complete\n",
      "2025-06-05 14:13:27,842 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\n",
      "2025-06-05 14:13:27,978 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x310d41390>\n",
      "2025-06-05 14:13:27,978 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x1202f6960> server_hostname='api.groq.com' timeout=600.0\n",
      "2025-06-05 14:13:28,004 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x30f2ea190>\n",
      "2025-06-05 14:13:28,006 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:28,007 - DEBUG - send_request_headers.complete\n",
      "2025-06-05 14:13:28,007 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:28,008 - DEBUG - send_request_body.complete\n",
      "2025-06-05 14:13:28,008 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:29,298 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 05 Jun 2025 13:13:29 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-europe-west3'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'999'), (b'x-ratelimit-remaining-tokens', b'28915'), (b'x-ratelimit-reset-requests', b'1m26.4s'), (b'x-ratelimit-reset-tokens', b'2.17s'), (b'x-request-id', b'req_01jx03k87nfxcrhqyc8ty6z8xp'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94afed6e2905d745-LHR'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2025-06-05 14:13:29,299 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 14:13:29,299 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:29,300 - DEBUG - receive_response_body.complete\n",
      "2025-06-05 14:13:29,300 - DEBUG - response_closed.started\n",
      "2025-06-05 14:13:29,301 - DEBUG - response_closed.complete\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-0a66e0f3-10c7-49b1-a8c6-89ceb0f4075a\", \"object\": \"chat.completion\", \"created\": 1749129208, \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Here are the extracted fields:\\n\\n**Name:** Vikram Bhat\\n\\n**Education:**\\n\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\n\\n**Skills:**\\n\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\n* Programming: Python, R, SQL\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\n\\n**Work Experience:**\\n\\n* **Data Scientist**, IBM, Dublin (May 2019 \\u2013 Present)\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\u2013 Apr 2019)\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\u2013 Aug 2016)\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.557457187, \"prompt_tokens\": 790, \"prompt_time\": 0.031091911, \"completion_tokens\": 271, \"completion_time\": 0.64942491, \"total_tokens\": 1061, \"total_time\": 0.680516821}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_37da608fc1\", \"x_groq\": {\"id\": \"req_01jx03k87nfxcrhqyc8ty6z8xp\"}}\n",
      "\n",
      "\n",
      "2025-06-05 14:13:29,301 - DEBUG - RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-0a66e0f3-10c7-49b1-a8c6-89ceb0f4075a\", \"object\": \"chat.completion\", \"created\": 1749129208, \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Here are the extracted fields:\\n\\n**Name:** Vikram Bhat\\n\\n**Education:**\\n\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\n\\n**Skills:**\\n\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\n* Programming: Python, R, SQL\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\n\\n**Work Experience:**\\n\\n* **Data Scientist**, IBM, Dublin (May 2019 \\u2013 Present)\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\u2013 Apr 2019)\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\u2013 Aug 2016)\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.557457187, \"prompt_tokens\": 790, \"prompt_time\": 0.031091911, \"completion_tokens\": 271, \"completion_time\": 0.64942491, \"total_tokens\": 1061, \"total_time\": 0.680516821}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_37da608fc1\", \"x_groq\": {\"id\": \"req_01jx03k87nfxcrhqyc8ty6z8xp\"}}\n",
      "\n",
      "\n",
      "\u001b[92m14:13:29 - LiteLLM:INFO\u001b[0m: utils.py:1211 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-05 14:13:29,302 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:29 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:13:29,303 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:29,304 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 8.69e-05, completion_tokens_cost_usd_dollar: 9.214e-05\n",
      "2025-06-05 14:13:29,306 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 8.69e-05, completion_tokens_cost_usd_dollar: 9.214e-05\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.00017904\n",
      "2025-06-05 14:13:29,307 - DEBUG - response_cost: 0.00017904\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \n",
      "\n",
      "2025-06-05 14:13:29,312 - DEBUG - \n",
      "\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "2025-06-05 14:13:29,314 - DEBUG - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \u001b[92mlitellm.completion(model='groq/meta-llama/llama-4-scout-17b-16e-instruct', messages=[{'role': 'user', 'content': \"\\n        You are a JD parser. Extract:\\n        - Job title\\n        - Responsibilities\\n        - Required/Preferred skills\\n\\n        Job Description:\\n        Primary Responsibilities    Analyze, review and interpret complex data through the development of case-studies, enabling the data to tell a story  Build and maintain production-ready machine-learning models  Identify drivers of patterns / behaviors uncovered in data and use these to explain business trends  Contribute to the design and realization of analytical based tools/assets to identify and monitor non-standard claim patterns  Engage with subject-matter experts to explore business problems and design solutions  Present analysis and interpretation for both operational and business review and planning  Support short and long term operational and strategic business activities through the use of data and develop recommended business solutions through research, analysis and implement when appropriate   You will be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role, as well as providing development for other roles you may be interested in.  Required Qualifications    Master's degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain with a significant quantitative aspect  Proficiency in AI/ML and deep learning frameworks such us PyTorch  Proven track record of developing and deploying machine learning models in production environments\\n\\n        \"}], temperature=0.7, stop=['\\nObservation:'], stream=False)\u001b[0m\n",
      "2025-06-05 14:13:29,315 - DEBUG - \u001b[92mlitellm.completion(model='groq/meta-llama/llama-4-scout-17b-16e-instruct', messages=[{'role': 'user', 'content': \"\\n        You are a JD parser. Extract:\\n        - Job title\\n        - Responsibilities\\n        - Required/Preferred skills\\n\\n        Job Description:\\n        Primary Responsibilities    Analyze, review and interpret complex data through the development of case-studies, enabling the data to tell a story  Build and maintain production-ready machine-learning models  Identify drivers of patterns / behaviors uncovered in data and use these to explain business trends  Contribute to the design and realization of analytical based tools/assets to identify and monitor non-standard claim patterns  Engage with subject-matter experts to explore business problems and design solutions  Present analysis and interpretation for both operational and business review and planning  Support short and long term operational and strategic business activities through the use of data and develop recommended business solutions through research, analysis and implement when appropriate   You will be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role, as well as providing development for other roles you may be interested in.  Required Qualifications    Master's degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain with a significant quantitative aspect  Proficiency in AI/ML and deep learning frameworks such us PyTorch  Proven track record of developing and deploying machine learning models in production environments\\n\\n        \"}], temperature=0.7, stop=['\\nObservation:'], stream=False)\u001b[0m\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \n",
      "\n",
      "2025-06-05 14:13:29,316 - DEBUG - \n",
      "\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:458 - self.optional_params: {}\n",
      "2025-06-05 14:13:29,318 - DEBUG - self.optional_params: {}\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "2025-06-05 14:13:29,318 - DEBUG - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m14:13:29 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-scout-17b-16e-instruct; provider = groq\n",
      "2025-06-05 14:13:29,320 - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-scout-17b-16e-instruct; provider = groq\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:2908 - \n",
      "LiteLLM: Params passed to completion() {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'functions': None, 'function_call': None, 'temperature': 0.7, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': ['\\nObservation:'], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are a JD parser. Extract:\\n        - Job title\\n        - Responsibilities\\n        - Required/Preferred skills\\n\\n        Job Description:\\n        Primary Responsibilities    Analyze, review and interpret complex data through the development of case-studies, enabling the data to tell a story  Build and maintain production-ready machine-learning models  Identify drivers of patterns / behaviors uncovered in data and use these to explain business trends  Contribute to the design and realization of analytical based tools/assets to identify and monitor non-standard claim patterns  Engage with subject-matter experts to explore business problems and design solutions  Present analysis and interpretation for both operational and business review and planning  Support short and long term operational and strategic business activities through the use of data and develop recommended business solutions through research, analysis and implement when appropriate   You will be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role, as well as providing development for other roles you may be interested in.  Required Qualifications    Master's degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain with a significant quantitative aspect  Proficiency in AI/ML and deep learning frameworks such us PyTorch  Proven track record of developing and deploying machine learning models in production environments\\n\\n        \"}], 'thinking': None, 'web_search_options': None}\n",
      "2025-06-05 14:13:29,320 - DEBUG - \n",
      "LiteLLM: Params passed to completion() {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'functions': None, 'function_call': None, 'temperature': 0.7, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': ['\\nObservation:'], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"\\n        You are a JD parser. Extract:\\n        - Job title\\n        - Responsibilities\\n        - Required/Preferred skills\\n\\n        Job Description:\\n        Primary Responsibilities    Analyze, review and interpret complex data through the development of case-studies, enabling the data to tell a story  Build and maintain production-ready machine-learning models  Identify drivers of patterns / behaviors uncovered in data and use these to explain business trends  Contribute to the design and realization of analytical based tools/assets to identify and monitor non-standard claim patterns  Engage with subject-matter experts to explore business problems and design solutions  Present analysis and interpretation for both operational and business review and planning  Support short and long term operational and strategic business activities through the use of data and develop recommended business solutions through research, analysis and implement when appropriate   You will be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role, as well as providing development for other roles you may be interested in.  Required Qualifications    Master's degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain with a significant quantitative aspect  Proficiency in AI/ML and deep learning frameworks such us PyTorch  Proven track record of developing and deploying machine learning models in production environments\\n\\n        \"}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:2911 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}\n",
      "2025-06-05 14:13:29,321 - DEBUG - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - Final returned optional params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "2025-06-05 14:13:29,322 - DEBUG - Final returned optional params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:458 - self.optional_params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "2025-06-05 14:13:29,322 - DEBUG - self.optional_params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "\u001b[92m14:13:29 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:901 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.groq.com/openai/v1/chat/completions \\\n",
      "-H 'Authorization: Be****An' -H 'Content-Type: ap****on' \\\n",
      "-d '{'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'messages': [{'role': 'user', 'content': \"\\n        You are a JD parser. Extract:\\n        - Job title\\n        - Responsibilities\\n        - Required/Preferred skills\\n\\n        Job Description:\\n        Primary Responsibilities    Analyze, review and interpret complex data through the development of case-studies, enabling the data to tell a story  Build and maintain production-ready machine-learning models  Identify drivers of patterns / behaviors uncovered in data and use these to explain business trends  Contribute to the design and realization of analytical based tools/assets to identify and monitor non-standard claim patterns  Engage with subject-matter experts to explore business problems and design solutions  Present analysis and interpretation for both operational and business review and planning  Support short and long term operational and strategic business activities through the use of data and develop recommended business solutions through research, analysis and implement when appropriate   You will be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role, as well as providing development for other roles you may be interested in.  Required Qualifications    Master's degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain with a significant quantitative aspect  Proficiency in AI/ML and deep learning frameworks such us PyTorch  Proven track record of developing and deploying machine learning models in production environments\\n\\n        \"}], 'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}'\n",
      "\u001b[0m\n",
      "\n",
      "2025-06-05 14:13:29,323 - DEBUG - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.groq.com/openai/v1/chat/completions \\\n",
      "-H 'Authorization: Be****An' -H 'Content-Type: ap****on' \\\n",
      "-d '{'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'messages': [{'role': 'user', 'content': \"\\n        You are a JD parser. Extract:\\n        - Job title\\n        - Responsibilities\\n        - Required/Preferred skills\\n\\n        Job Description:\\n        Primary Responsibilities    Analyze, review and interpret complex data through the development of case-studies, enabling the data to tell a story  Build and maintain production-ready machine-learning models  Identify drivers of patterns / behaviors uncovered in data and use these to explain business trends  Contribute to the design and realization of analytical based tools/assets to identify and monitor non-standard claim patterns  Engage with subject-matter experts to explore business problems and design solutions  Present analysis and interpretation for both operational and business review and planning  Support short and long term operational and strategic business activities through the use of data and develop recommended business solutions through research, analysis and implement when appropriate   You will be rewarded and recognized for your performance in an environment that will challenge you and give you clear direction on what it takes to succeed in your role, as well as providing development for other roles you may be interested in.  Required Qualifications    Master's degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain with a significant quantitative aspect  Proficiency in AI/ML and deep learning frameworks such us PyTorch  Proven track record of developing and deploying machine learning models in production environments\\n\\n        \"}], 'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}'\n",
      "\u001b[0m\n",
      "\n",
      "2025-06-05 14:13:29,324 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:29,324 - DEBUG - send_request_headers.complete\n",
      "2025-06-05 14:13:29,325 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:29,325 - DEBUG - send_request_body.complete\n",
      "2025-06-05 14:13:29,326 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:30,240 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 05 Jun 2025 13:13:30 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-europe-west3'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'998'), (b'x-ratelimit-remaining-tokens', b'28574'), (b'x-ratelimit-reset-requests', b'2m51.494s'), (b'x-ratelimit-reset-tokens', b'2.852s'), (b'x-request-id', b'req_01jx03k9gkf7xr8mprm9ff26wg'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94afed766b7ad745-LHR'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2025-06-05 14:13:30,240 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 14:13:30,241 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:30,241 - DEBUG - receive_response_body.complete\n",
      "2025-06-05 14:13:30,242 - DEBUG - response_closed.started\n",
      "2025-06-05 14:13:30,242 - DEBUG - response_closed.complete\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-4b7054fb-25fe-4b24-b445-b22331f88133\", \"object\": \"chat.completion\", \"created\": 1749129209, \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Here are the extracted information:\\n\\n**Job Title:** \\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\n\\n**Responsibilities:**\\n\\n1. Analyze, review and interpret complex data through case-studies\\n2. Build and maintain production-ready machine-learning models\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\n4. Contribute to designing and realizing analytical tools/assets\\n5. Engage with subject-matter experts to explore business problems and design solutions\\n6. Present analysis and interpretation for operational and business review and planning\\n7. Support short and long term operational and strategic business activities through data analysis\\n\\n**Required/Preferred Skills:**\\n\\n1. **Education:** Master's degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\n2. **Technical Skills:**\\n\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\n\\t* Experience in developing and deploying machine learning models in production environments\\n3. **Analytical Skills:**\\n\\t* Strong quantitative analysis and problem-solving skills\\n\\t* Ability to interpret complex data and identify patterns/behaviors.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.195075016, \"prompt_tokens\": 258, \"prompt_time\": 0.014562295, \"completion_tokens\": 234, \"completion_time\": 0.561072521, \"total_tokens\": 492, \"total_time\": 0.575634816}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_37da608fc1\", \"x_groq\": {\"id\": \"req_01jx03k9gkf7xr8mprm9ff26wg\"}}\n",
      "\n",
      "\n",
      "2025-06-05 14:13:30,242 - DEBUG - RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-4b7054fb-25fe-4b24-b445-b22331f88133\", \"object\": \"chat.completion\", \"created\": 1749129209, \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Here are the extracted information:\\n\\n**Job Title:** \\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\n\\n**Responsibilities:**\\n\\n1. Analyze, review and interpret complex data through case-studies\\n2. Build and maintain production-ready machine-learning models\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\n4. Contribute to designing and realizing analytical tools/assets\\n5. Engage with subject-matter experts to explore business problems and design solutions\\n6. Present analysis and interpretation for operational and business review and planning\\n7. Support short and long term operational and strategic business activities through data analysis\\n\\n**Required/Preferred Skills:**\\n\\n1. **Education:** Master's degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\n2. **Technical Skills:**\\n\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\n\\t* Experience in developing and deploying machine learning models in production environments\\n3. **Analytical Skills:**\\n\\t* Strong quantitative analysis and problem-solving skills\\n\\t* Ability to interpret complex data and identify patterns/behaviors.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.195075016, \"prompt_tokens\": 258, \"prompt_time\": 0.014562295, \"completion_tokens\": 234, \"completion_time\": 0.561072521, \"total_tokens\": 492, \"total_time\": 0.575634816}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_37da608fc1\", \"x_groq\": {\"id\": \"req_01jx03k9gkf7xr8mprm9ff26wg\"}}\n",
      "\n",
      "\n",
      "\u001b[92m14:13:30 - LiteLLM:INFO\u001b[0m: utils.py:1211 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-05 14:13:30,244 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:13:30,245 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:30,246 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 2.8380000000000003e-05, completion_tokens_cost_usd_dollar: 7.955999999999999e-05\n",
      "2025-06-05 14:13:30,247 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 2.8380000000000003e-05, completion_tokens_cost_usd_dollar: 7.955999999999999e-05\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.00010794\n",
      "2025-06-05 14:13:30,248 - DEBUG - response_cost: 0.00010794\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \n",
      "\n",
      "2025-06-05 14:13:30,254 - DEBUG - \n",
      "\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "2025-06-05 14:13:30,256 - DEBUG - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \u001b[92mlitellm.completion(model='groq/meta-llama/llama-4-scout-17b-16e-instruct', messages=[{'role': 'user', 'content': '\\n        Compare the following resume and job_description info.\\n        List skills or experiences that MATCH and those that are MISSING(present in Job Description but not in Resume ).\\n        Respond as : {\"matched_skills\": [], \"missing_skills\": [], \"match_score\": 0.0}\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}        \\n        '}], temperature=0.7, stop=['\\nObservation:'], stream=False)\u001b[0m\n",
      "2025-06-05 14:13:30,257 - DEBUG - \u001b[92mlitellm.completion(model='groq/meta-llama/llama-4-scout-17b-16e-instruct', messages=[{'role': 'user', 'content': '\\n        Compare the following resume and job_description info.\\n        List skills or experiences that MATCH and those that are MISSING(present in Job Description but not in Resume ).\\n        Respond as : {\"matched_skills\": [], \"missing_skills\": [], \"match_score\": 0.0}\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}        \\n        '}], temperature=0.7, stop=['\\nObservation:'], stream=False)\u001b[0m\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \n",
      "\n",
      "2025-06-05 14:13:30,258 - DEBUG - \n",
      "\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:458 - self.optional_params: {}\n",
      "2025-06-05 14:13:30,260 - DEBUG - self.optional_params: {}\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "2025-06-05 14:13:30,261 - DEBUG - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m14:13:30 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-scout-17b-16e-instruct; provider = groq\n",
      "2025-06-05 14:13:30,262 - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-scout-17b-16e-instruct; provider = groq\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:2908 - \n",
      "LiteLLM: Params passed to completion() {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'functions': None, 'function_call': None, 'temperature': 0.7, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': ['\\nObservation:'], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': '\\n        Compare the following resume and job_description info.\\n        List skills or experiences that MATCH and those that are MISSING(present in Job Description but not in Resume ).\\n        Respond as : {\"matched_skills\": [], \"missing_skills\": [], \"match_score\": 0.0}\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}        \\n        '}], 'thinking': None, 'web_search_options': None}\n",
      "2025-06-05 14:13:30,263 - DEBUG - \n",
      "LiteLLM: Params passed to completion() {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'functions': None, 'function_call': None, 'temperature': 0.7, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': ['\\nObservation:'], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': '\\n        Compare the following resume and job_description info.\\n        List skills or experiences that MATCH and those that are MISSING(present in Job Description but not in Resume ).\\n        Respond as : {\"matched_skills\": [], \"missing_skills\": [], \"match_score\": 0.0}\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}        \\n        '}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:2911 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}\n",
      "2025-06-05 14:13:30,263 - DEBUG - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - Final returned optional params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "2025-06-05 14:13:30,264 - DEBUG - Final returned optional params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:458 - self.optional_params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "2025-06-05 14:13:30,264 - DEBUG - self.optional_params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "\u001b[92m14:13:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:901 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.groq.com/openai/v1/chat/completions \\\n",
      "-H 'Authorization: Be****An' -H 'Content-Type: ap****on' \\\n",
      "-d '{'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'messages': [{'role': 'user', 'content': '\\n        Compare the following resume and job_description info.\\n        List skills or experiences that MATCH and those that are MISSING(present in Job Description but not in Resume ).\\n        Respond as : {\"matched_skills\": [], \"missing_skills\": [], \"match_score\": 0.0}\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}        \\n        '}], 'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}'\n",
      "\u001b[0m\n",
      "\n",
      "2025-06-05 14:13:30,265 - DEBUG - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.groq.com/openai/v1/chat/completions \\\n",
      "-H 'Authorization: Be****An' -H 'Content-Type: ap****on' \\\n",
      "-d '{'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'messages': [{'role': 'user', 'content': '\\n        Compare the following resume and job_description info.\\n        List skills or experiences that MATCH and those that are MISSING(present in Job Description but not in Resume ).\\n        Respond as : {\"matched_skills\": [], \"missing_skills\": [], \"match_score\": 0.0}\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}        \\n        '}], 'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}'\n",
      "\u001b[0m\n",
      "\n",
      "2025-06-05 14:13:30,266 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:30,267 - DEBUG - send_request_headers.complete\n",
      "2025-06-05 14:13:30,267 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:30,268 - DEBUG - send_request_body.complete\n",
      "2025-06-05 14:13:30,268 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug - MatchingAgent context type: <class 'str'>\n",
      "Debug - MatchingAgent context content: {\"resume\": \"Here are the extracted fields:\\n\\n**Name:** Vikram Bhat\\n\\n**Education:**\\n\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\n\\n**Skills:**\\n\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\n* Programming: Python, R, SQL\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\n\\n**Work Experience:**\\n\\n* **Data Scientist**, IBM, Dublin (May 2019 \\u2013 Present)\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\u2013 Apr 2019)\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\u2013 Aug 2016)\"}\n",
      "\n",
      "----------\n",
      "\n",
      "{\"job_description\": \"Here are the extracted information:\\n\\n**Job Title:** \\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\n\\n**Responsibilities:**\\n\\n1. Analyze, review and interpret complex data through case-studies\\n2. Build and maintain production-ready machine-learning models\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\n4. Contribute to designing and realizing analytical tools/assets\\n5. Engage with subject-matter experts to explore business problems and design solutions\\n6. Present analysis and interpretation for operational and business review and planning\\n7. Support short and long term operational and strategic business activities through data analysis\\n\\n**Required/Preferred Skills:**\\n\\n1. **Education:** Master's degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\n2. **Technical Skills:**\\n\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\n\\t* Experience in developing and deploying machine learning models in production environments\\n3. **Analytical Skills:**\\n\\t* Strong quantitative analysis and problem-solving skills\\n\\t* Ability to interpret complex data and identify patterns/behaviors.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-05 14:13:31,468 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 05 Jun 2025 13:13:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-europe-west3'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'997'), (b'x-ratelimit-remaining-tokens', b'28307'), (b'x-ratelimit-reset-requests', b'4m18.183999999s'), (b'x-ratelimit-reset-tokens', b'3.386s'), (b'x-request-id', b'req_01jx03kagbf81rqj68pa9cz9hw'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94afed7cb87ad745-LHR'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2025-06-05 14:13:31,469 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 14:13:31,469 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:31,470 - DEBUG - receive_response_body.complete\n",
      "2025-06-05 14:13:31,471 - DEBUG - response_closed.started\n",
      "2025-06-05 14:13:31,471 - DEBUG - response_closed.complete\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-926cc3bb-d160-4ff1-bb2d-8d3058a172c9\", \"object\": \"chat.completion\", \"created\": 1749129210, \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Here is the comparison of the resume and job description:\\n\\n```\\n{\\n  \\\"matched_skills\\\": [\\n    \\\"Machine Learning & AI\\\",\\n    \\\"Deep Learning Frameworks (PyTorch, TensorFlow)\\\",\\n    \\\"Data Analysis\\\",\\n    \\\"Problem-solving\\\",\\n    \\\"Interpretation of complex data\\\"\\n  ],\\n  \\\"missing_skills\\\": [\\n    \\\"Experience in developing and deploying machine learning models in production environments (explicitly mentioned in Job Description)\\\",\\n    \\\"Master's degree in Statistics, Physics, Mathematics, or similar analytic domain (specific domain mentioned in Job Description)\\\"\\n  ],\\n  \\\"match_score\\\": 0.8\\n}\\n```\\n\\nHere's a brief explanation:\\n\\n**Matched Skills:**\\n\\n* Machine Learning & AI (resume) matches with AI/ML and deep learning frameworks (job description)\\n* Deep Learning Frameworks (PyTorch, TensorFlow) mentioned in resume\\n* Data Analysis, Problem-solving, and Interpretation of complex data are all implied in the resume's work experience and skills\\n\\n**Missing Skills:**\\n\\n* Explicit experience in developing and deploying machine learning models in production environments (mentioned in job description but not explicitly mentioned in resume, although there is a mention of building and maintaining production-ready machine-learning models in work experience)\\n* Specific Master's degree domain (Statistics, Physics, Mathematics) mentioned in job description, whereas the resume has a related but different domain (Data Science and Analytics)\\n\\n**Match Score:** \\nI have assigned a match score of 0.8, indicating a high level of matching between the resume and job description. However, this score is subjective and may vary based on individual evaluation. \\n\\nNote that this comparison is not exhaustive, and a more detailed analysis might be required for a comprehensive evaluation.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.13509879400000002, \"prompt_tokens\": 619, \"prompt_time\": 0.036960856, \"completion_tokens\": 343, \"completion_time\": 0.820157273, \"total_tokens\": 962, \"total_time\": 0.857118129}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_37da608fc1\", \"x_groq\": {\"id\": \"req_01jx03kagbf81rqj68pa9cz9hw\"}}\n",
      "\n",
      "\n",
      "2025-06-05 14:13:31,472 - DEBUG - RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-926cc3bb-d160-4ff1-bb2d-8d3058a172c9\", \"object\": \"chat.completion\", \"created\": 1749129210, \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Here is the comparison of the resume and job description:\\n\\n```\\n{\\n  \\\"matched_skills\\\": [\\n    \\\"Machine Learning & AI\\\",\\n    \\\"Deep Learning Frameworks (PyTorch, TensorFlow)\\\",\\n    \\\"Data Analysis\\\",\\n    \\\"Problem-solving\\\",\\n    \\\"Interpretation of complex data\\\"\\n  ],\\n  \\\"missing_skills\\\": [\\n    \\\"Experience in developing and deploying machine learning models in production environments (explicitly mentioned in Job Description)\\\",\\n    \\\"Master's degree in Statistics, Physics, Mathematics, or similar analytic domain (specific domain mentioned in Job Description)\\\"\\n  ],\\n  \\\"match_score\\\": 0.8\\n}\\n```\\n\\nHere's a brief explanation:\\n\\n**Matched Skills:**\\n\\n* Machine Learning & AI (resume) matches with AI/ML and deep learning frameworks (job description)\\n* Deep Learning Frameworks (PyTorch, TensorFlow) mentioned in resume\\n* Data Analysis, Problem-solving, and Interpretation of complex data are all implied in the resume's work experience and skills\\n\\n**Missing Skills:**\\n\\n* Explicit experience in developing and deploying machine learning models in production environments (mentioned in job description but not explicitly mentioned in resume, although there is a mention of building and maintaining production-ready machine-learning models in work experience)\\n* Specific Master's degree domain (Statistics, Physics, Mathematics) mentioned in job description, whereas the resume has a related but different domain (Data Science and Analytics)\\n\\n**Match Score:** \\nI have assigned a match score of 0.8, indicating a high level of matching between the resume and job description. However, this score is subjective and may vary based on individual evaluation. \\n\\nNote that this comparison is not exhaustive, and a more detailed analysis might be required for a comprehensive evaluation.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.13509879400000002, \"prompt_tokens\": 619, \"prompt_time\": 0.036960856, \"completion_tokens\": 343, \"completion_time\": 0.820157273, \"total_tokens\": 962, \"total_time\": 0.857118129}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_37da608fc1\", \"x_groq\": {\"id\": \"req_01jx03kagbf81rqj68pa9cz9hw\"}}\n",
      "\n",
      "\n",
      "\u001b[92m14:13:31 - LiteLLM:INFO\u001b[0m: utils.py:1211 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-05 14:13:31,474 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:13:31,474 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:31,476 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.809000000000001e-05, completion_tokens_cost_usd_dollar: 0.00011661999999999999\n",
      "2025-06-05 14:13:31,477 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.809000000000001e-05, completion_tokens_cost_usd_dollar: 0.00011661999999999999\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.00018470999999999998\n",
      "2025-06-05 14:13:31,479 - DEBUG - response_cost: 0.00018470999999999998\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \n",
      "\n",
      "2025-06-05 14:13:31,484 - DEBUG - \n",
      "\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "2025-06-05 14:13:31,486 - DEBUG - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \u001b[92mlitellm.completion(model='groq/meta-llama/llama-4-scout-17b-16e-instruct', messages=[{'role': 'user', 'content': '\\n        Given this resume and job description, suggest improvements to the resume:\\n        - Highlight missing skills\\n        - Recommend better phrasing\\n        - Suggest added sections or content\\n\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}\\n        '}], temperature=0.7, stop=['\\nObservation:'], stream=False)\u001b[0m\n",
      "2025-06-05 14:13:31,487 - DEBUG - \u001b[92mlitellm.completion(model='groq/meta-llama/llama-4-scout-17b-16e-instruct', messages=[{'role': 'user', 'content': '\\n        Given this resume and job description, suggest improvements to the resume:\\n        - Highlight missing skills\\n        - Recommend better phrasing\\n        - Suggest added sections or content\\n\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}\\n        '}], temperature=0.7, stop=['\\nObservation:'], stream=False)\u001b[0m\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \n",
      "\n",
      "2025-06-05 14:13:31,488 - DEBUG - \n",
      "\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:458 - self.optional_params: {}\n",
      "2025-06-05 14:13:31,489 - DEBUG - self.optional_params: {}\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "2025-06-05 14:13:31,490 - DEBUG - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m14:13:31 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-scout-17b-16e-instruct; provider = groq\n",
      "2025-06-05 14:13:31,491 - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-scout-17b-16e-instruct; provider = groq\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:2908 - \n",
      "LiteLLM: Params passed to completion() {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'functions': None, 'function_call': None, 'temperature': 0.7, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': ['\\nObservation:'], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': '\\n        Given this resume and job description, suggest improvements to the resume:\\n        - Highlight missing skills\\n        - Recommend better phrasing\\n        - Suggest added sections or content\\n\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}\\n        '}], 'thinking': None, 'web_search_options': None}\n",
      "2025-06-05 14:13:31,493 - DEBUG - \n",
      "LiteLLM: Params passed to completion() {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'functions': None, 'function_call': None, 'temperature': 0.7, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': ['\\nObservation:'], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': '\\n        Given this resume and job description, suggest improvements to the resume:\\n        - Highlight missing skills\\n        - Recommend better phrasing\\n        - Suggest added sections or content\\n\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}\\n        '}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:2911 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}\n",
      "2025-06-05 14:13:31,494 - DEBUG - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - Final returned optional params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "2025-06-05 14:13:31,494 - DEBUG - Final returned optional params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:458 - self.optional_params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "2025-06-05 14:13:31,495 - DEBUG - self.optional_params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "\u001b[92m14:13:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:901 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.groq.com/openai/v1/chat/completions \\\n",
      "-H 'Authorization: Be****An' -H 'Content-Type: ap****on' \\\n",
      "-d '{'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'messages': [{'role': 'user', 'content': '\\n        Given this resume and job description, suggest improvements to the resume:\\n        - Highlight missing skills\\n        - Recommend better phrasing\\n        - Suggest added sections or content\\n\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}\\n        '}], 'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}'\n",
      "\u001b[0m\n",
      "\n",
      "2025-06-05 14:13:31,496 - DEBUG - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.groq.com/openai/v1/chat/completions \\\n",
      "-H 'Authorization: Be****An' -H 'Content-Type: ap****on' \\\n",
      "-d '{'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'messages': [{'role': 'user', 'content': '\\n        Given this resume and job description, suggest improvements to the resume:\\n        - Highlight missing skills\\n        - Recommend better phrasing\\n        - Suggest added sections or content\\n\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}\\n        '}], 'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}'\n",
      "\u001b[0m\n",
      "\n",
      "2025-06-05 14:13:31,497 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:31,498 - DEBUG - send_request_headers.complete\n",
      "2025-06-05 14:13:31,498 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:31,498 - DEBUG - send_request_body.complete\n",
      "2025-06-05 14:13:31,499 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:34,132 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 05 Jun 2025 13:13:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-europe-west3'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'996'), (b'x-ratelimit-remaining-tokens', b'27942'), (b'x-ratelimit-reset-requests', b'5m44.444999999s'), (b'x-ratelimit-reset-tokens', b'4.115s'), (b'x-request-id', b'req_01jx03kbmefxms9weyx1p97jan'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94afed83fe91d745-LHR'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2025-06-05 14:13:34,134 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 14:13:34,135 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:34,137 - DEBUG - receive_response_body.complete\n",
      "2025-06-05 14:13:34,137 - DEBUG - response_closed.started\n",
      "2025-06-05 14:13:34,138 - DEBUG - response_closed.complete\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-503eb07b-ec8d-4352-b555-fa858e5aca3f\", \"object\": \"chat.completion\", \"created\": 1749129211, \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Based on the provided resume and job description, here are some suggestions for improvements:\\n\\n**Missing Skills:**\\n\\n* None explicitly mentioned, but it's a good idea to include \\\"Statistics\\\" or \\\"Mathematical Modeling\\\" in the skills section, given the job description's emphasis on these areas.\\n\\n**Better Phrasing:**\\n\\n* Instead of \\\"Machine Learning & AI: Supervised & Unsupervised Learning...\\\", consider rephrasing to:\\n\\t+ \\\"Artificial Intelligence & Machine Learning: Expertise in developing and deploying supervised and unsupervised learning models, with a focus on...\\\"\\n\\t+ This rephrasing makes the skill more concise and impactful.\\n* For \\\"Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\", consider:\\n\\t+ \\\"Data Engineering: Proficient in designing and implementing data pipelines using Elasticsearch, Milvus, and ETL tools, with expertise in data warehousing...\\\"\\n\\t+ This rephrasing highlights the candidate's ability to design and implement data pipelines.\\n\\n**Added Sections or Content:**\\n\\n* **Projects:** Consider adding a separate section to showcase relevant projects that demonstrate the candidate's skills, such as:\\n\\t+ A brief description of a project where you developed and deployed a machine learning model in a production environment.\\n\\t+ A project that showcases your expertise in data analysis and interpretation, highlighting your ability to identify patterns and behaviors in complex data.\\n* **Achievements:** Add a section to highlight achievements and accomplishments in previous roles, such as:\\n\\t+ \\\"Improved model accuracy by 25% through hyperparameter tuning and feature engineering\\\" ( Data Scientist, IBM)\\n\\t+ \\\"Developed and deployed a predictive maintenance model that reduced equipment downtime by 30%\\\" ( Data Analyst, Voxpro Groups)\\n* **Certifications:** If relevant, consider adding any relevant certifications, such as Certified Data Scientist or Certified Machine Learning Engineer.\\n\\n**Reorganized Skills Section:**\\n\\nConsider reorganizing the skills section to group similar skills together:\\n\\n* **Programming:** Python, R, SQL\\n* **Machine Learning & AI:** \\n\\t+ Supervised & Unsupervised Learning\\n\\t+ Time Series Forecasting\\n\\t+ NLP\\n\\t+ Large Language Models (LLMs)\\n\\t+ Retrieval-Augmented Generation (RAG)\\n\\t+ Prompt Engineering\\n\\t+ Model Evaluation & Safety (bias, hallucination detection)\\n* **Deep Learning Frameworks:** TensorFlow, PyTorch, Hugging Face Transformers\\n* **Data Engineering:** Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\n* **Cloud:** AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\n* **Visualization:** R Shiny, Power BI, Streamlit, Gradio, Dash\\n\\n**Updated Resume:**\\n\\nHere is an updated version of the resume incorporating the suggested improvements:\\n\\n**Name:** Vikram Bhat\\n\\n**Education:**\\n\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\n\\n**Skills:**\\n\\n* **Programming:** Python, R, SQL\\n* **Machine Learning & AI:** \\n\\t+ Supervised & Unsupervised Learning\\n\\t+ Time Series Forecasting\\n\\t+ NLP\\n\\t+ Large Language Models (LLMs)\\n\\t+ Retrieval-Augmented Generation (RAG)\\n\\t+ Prompt Engineering\\n\\t+ Model Evaluation & Safety (bias, hallucination detection)\\n* **Deep Learning Frameworks:** TensorFlow, PyTorch, Hugging Face Transformers\\n* **Data Engineering:** Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\n* **Cloud:** AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\n* **Visualization:** R Shiny, Power BI, Streamlit, Gradio, Dash\\n\\n**Work Experience:**\\n\\n* **Data Scientist**, IBM, Dublin (May 2019 \\u2013 Present)\\n\\t+ Analyzed complex data to identify patterns and behaviors, and developed predictive models to drive business decisions.\\n\\t+ Deployed machine learning models in production environments, ensuring scalability and reliability.\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\u2013 Apr 2019)\\n\\t+ Developed and deployed a predictive maintenance model that reduced equipment downtime by 30%.\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\u2013 Aug 2016)\\n\\n**Achievements:**\\n\\n* Improved model accuracy by 25% through hyperparameter tuning and feature engineering (Data Scientist, IBM)\\n* Developed and deployed a predictive maintenance model that reduced equipment downtime by 30% (Data Analyst, Voxpro Groups)\\n\\n**Projects:** \\n\\n* Briefly describe a relevant project that demonstrates your skills in machine learning, data analysis, or data engineering.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.23161893900000002, \"prompt_tokens\": 596, \"prompt_time\": 0.03259922, \"completion_tokens\": 974, \"completion_time\": 2.327538365, \"total_tokens\": 1570, \"total_time\": 2.360137585}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_37da608fc1\", \"x_groq\": {\"id\": \"req_01jx03kbmefxms9weyx1p97jan\"}}\n",
      "\n",
      "\n",
      "2025-06-05 14:13:34,139 - DEBUG - RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-503eb07b-ec8d-4352-b555-fa858e5aca3f\", \"object\": \"chat.completion\", \"created\": 1749129211, \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Based on the provided resume and job description, here are some suggestions for improvements:\\n\\n**Missing Skills:**\\n\\n* None explicitly mentioned, but it's a good idea to include \\\"Statistics\\\" or \\\"Mathematical Modeling\\\" in the skills section, given the job description's emphasis on these areas.\\n\\n**Better Phrasing:**\\n\\n* Instead of \\\"Machine Learning & AI: Supervised & Unsupervised Learning...\\\", consider rephrasing to:\\n\\t+ \\\"Artificial Intelligence & Machine Learning: Expertise in developing and deploying supervised and unsupervised learning models, with a focus on...\\\"\\n\\t+ This rephrasing makes the skill more concise and impactful.\\n* For \\\"Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\", consider:\\n\\t+ \\\"Data Engineering: Proficient in designing and implementing data pipelines using Elasticsearch, Milvus, and ETL tools, with expertise in data warehousing...\\\"\\n\\t+ This rephrasing highlights the candidate's ability to design and implement data pipelines.\\n\\n**Added Sections or Content:**\\n\\n* **Projects:** Consider adding a separate section to showcase relevant projects that demonstrate the candidate's skills, such as:\\n\\t+ A brief description of a project where you developed and deployed a machine learning model in a production environment.\\n\\t+ A project that showcases your expertise in data analysis and interpretation, highlighting your ability to identify patterns and behaviors in complex data.\\n* **Achievements:** Add a section to highlight achievements and accomplishments in previous roles, such as:\\n\\t+ \\\"Improved model accuracy by 25% through hyperparameter tuning and feature engineering\\\" ( Data Scientist, IBM)\\n\\t+ \\\"Developed and deployed a predictive maintenance model that reduced equipment downtime by 30%\\\" ( Data Analyst, Voxpro Groups)\\n* **Certifications:** If relevant, consider adding any relevant certifications, such as Certified Data Scientist or Certified Machine Learning Engineer.\\n\\n**Reorganized Skills Section:**\\n\\nConsider reorganizing the skills section to group similar skills together:\\n\\n* **Programming:** Python, R, SQL\\n* **Machine Learning & AI:** \\n\\t+ Supervised & Unsupervised Learning\\n\\t+ Time Series Forecasting\\n\\t+ NLP\\n\\t+ Large Language Models (LLMs)\\n\\t+ Retrieval-Augmented Generation (RAG)\\n\\t+ Prompt Engineering\\n\\t+ Model Evaluation & Safety (bias, hallucination detection)\\n* **Deep Learning Frameworks:** TensorFlow, PyTorch, Hugging Face Transformers\\n* **Data Engineering:** Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\n* **Cloud:** AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\n* **Visualization:** R Shiny, Power BI, Streamlit, Gradio, Dash\\n\\n**Updated Resume:**\\n\\nHere is an updated version of the resume incorporating the suggested improvements:\\n\\n**Name:** Vikram Bhat\\n\\n**Education:**\\n\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\n\\n**Skills:**\\n\\n* **Programming:** Python, R, SQL\\n* **Machine Learning & AI:** \\n\\t+ Supervised & Unsupervised Learning\\n\\t+ Time Series Forecasting\\n\\t+ NLP\\n\\t+ Large Language Models (LLMs)\\n\\t+ Retrieval-Augmented Generation (RAG)\\n\\t+ Prompt Engineering\\n\\t+ Model Evaluation & Safety (bias, hallucination detection)\\n* **Deep Learning Frameworks:** TensorFlow, PyTorch, Hugging Face Transformers\\n* **Data Engineering:** Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\n* **Cloud:** AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\n* **Visualization:** R Shiny, Power BI, Streamlit, Gradio, Dash\\n\\n**Work Experience:**\\n\\n* **Data Scientist**, IBM, Dublin (May 2019 \\u2013 Present)\\n\\t+ Analyzed complex data to identify patterns and behaviors, and developed predictive models to drive business decisions.\\n\\t+ Deployed machine learning models in production environments, ensuring scalability and reliability.\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\u2013 Apr 2019)\\n\\t+ Developed and deployed a predictive maintenance model that reduced equipment downtime by 30%.\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\u2013 Aug 2016)\\n\\n**Achievements:**\\n\\n* Improved model accuracy by 25% through hyperparameter tuning and feature engineering (Data Scientist, IBM)\\n* Developed and deployed a predictive maintenance model that reduced equipment downtime by 30% (Data Analyst, Voxpro Groups)\\n\\n**Projects:** \\n\\n* Briefly describe a relevant project that demonstrates your skills in machine learning, data analysis, or data engineering.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.23161893900000002, \"prompt_tokens\": 596, \"prompt_time\": 0.03259922, \"completion_tokens\": 974, \"completion_time\": 2.327538365, \"total_tokens\": 1570, \"total_time\": 2.360137585}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_37da608fc1\", \"x_groq\": {\"id\": \"req_01jx03kbmefxms9weyx1p97jan\"}}\n",
      "\n",
      "\n",
      "\u001b[92m14:13:34 - LiteLLM:INFO\u001b[0m: utils.py:1211 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-05 14:13:34,140 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:13:34,142 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:34,143 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.556e-05, completion_tokens_cost_usd_dollar: 0.00033116\n",
      "2025-06-05 14:13:34,145 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.556e-05, completion_tokens_cost_usd_dollar: 0.00033116\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.00039672\n",
      "2025-06-05 14:13:34,147 - DEBUG - response_cost: 0.00039672\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \n",
      "\n",
      "2025-06-05 14:13:34,153 - DEBUG - \n",
      "\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \u001b[92mRequest to litellm:\u001b[0m\n",
      "2025-06-05 14:13:34,154 - DEBUG - \u001b[92mRequest to litellm:\u001b[0m\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \u001b[92mlitellm.completion(model='groq/meta-llama/llama-4-scout-17b-16e-instruct', messages=[{'role': 'user', 'content': '\\n        Generate a professional cover letter that:\\n        - Highlights the candidate\\'s strengths\\n        - Acknowledges and bridges skill or experience gaps\\n        - Aligns with the job description and tone\\n\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}\\n        '}], temperature=0.7, stop=['\\nObservation:'], stream=False)\u001b[0m\n",
      "2025-06-05 14:13:34,156 - DEBUG - \u001b[92mlitellm.completion(model='groq/meta-llama/llama-4-scout-17b-16e-instruct', messages=[{'role': 'user', 'content': '\\n        Generate a professional cover letter that:\\n        - Highlights the candidate\\'s strengths\\n        - Acknowledges and bridges skill or experience gaps\\n        - Aligns with the job description and tone\\n\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}\\n        '}], temperature=0.7, stop=['\\nObservation:'], stream=False)\u001b[0m\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - \n",
      "\n",
      "2025-06-05 14:13:34,158 - DEBUG - \n",
      "\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:458 - self.optional_params: {}\n",
      "2025-06-05 14:13:34,159 - DEBUG - self.optional_params: {}\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "2025-06-05 14:13:34,160 - DEBUG - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
      "\u001b[92m14:13:34 - LiteLLM:INFO\u001b[0m: utils.py:2905 - \n",
      "LiteLLM completion() model= meta-llama/llama-4-scout-17b-16e-instruct; provider = groq\n",
      "2025-06-05 14:13:34,162 - INFO - \n",
      "LiteLLM completion() model= meta-llama/llama-4-scout-17b-16e-instruct; provider = groq\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:2908 - \n",
      "LiteLLM: Params passed to completion() {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'functions': None, 'function_call': None, 'temperature': 0.7, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': ['\\nObservation:'], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': '\\n        Generate a professional cover letter that:\\n        - Highlights the candidate\\'s strengths\\n        - Acknowledges and bridges skill or experience gaps\\n        - Aligns with the job description and tone\\n\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}\\n        '}], 'thinking': None, 'web_search_options': None}\n",
      "2025-06-05 14:13:34,164 - DEBUG - \n",
      "LiteLLM: Params passed to completion() {'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'functions': None, 'function_call': None, 'temperature': 0.7, 'top_p': None, 'n': None, 'stream': False, 'stream_options': None, 'stop': ['\\nObservation:'], 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': '\\n        Generate a professional cover letter that:\\n        - Highlights the candidate\\'s strengths\\n        - Acknowledges and bridges skill or experience gaps\\n        - Aligns with the job description and tone\\n\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}\\n        '}], 'thinking': None, 'web_search_options': None}\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:2911 - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}\n",
      "2025-06-05 14:13:34,166 - DEBUG - \n",
      "LiteLLM: Non-Default params passed to completion() {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - Final returned optional params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "2025-06-05 14:13:34,167 - DEBUG - Final returned optional params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:458 - self.optional_params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "2025-06-05 14:13:34,168 - DEBUG - self.optional_params: {'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:'], 'extra_body': {}}\n",
      "\u001b[92m14:13:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:901 - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.groq.com/openai/v1/chat/completions \\\n",
      "-H 'Authorization: Be****An' -H 'Content-Type: ap****on' \\\n",
      "-d '{'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'messages': [{'role': 'user', 'content': '\\n        Generate a professional cover letter that:\\n        - Highlights the candidate\\'s strengths\\n        - Acknowledges and bridges skill or experience gaps\\n        - Aligns with the job description and tone\\n\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}\\n        '}], 'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}'\n",
      "\u001b[0m\n",
      "\n",
      "2025-06-05 14:13:34,169 - DEBUG - \u001b[92m\n",
      "\n",
      "POST Request Sent from LiteLLM:\n",
      "curl -X POST \\\n",
      "https://api.groq.com/openai/v1/chat/completions \\\n",
      "-H 'Authorization: Be****An' -H 'Content-Type: ap****on' \\\n",
      "-d '{'model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'messages': [{'role': 'user', 'content': '\\n        Generate a professional cover letter that:\\n        - Highlights the candidate\\'s strengths\\n        - Acknowledges and bridges skill or experience gaps\\n        - Aligns with the job description and tone\\n\\n        context: {\"resume\": \"Here are the extracted fields:\\\\n\\\\n**Name:** Vikram Bhat\\\\n\\\\n**Education:**\\\\n\\\\n* **MSc in Data Science and Analytics**, University College Cork, Cork (Graduated: October 2017, GPA: 1:1)\\\\n* **B.E. in Computer Science**, Sir M.VIT, Bengaluru (Graduated: June 2012)\\\\n\\\\n**Skills:**\\\\n\\\\n* Machine Learning & AI: Supervised & Unsupervised Learning, Time Series Forecasting, NLP, Large Language Models (LLMs), Retrieval-Augmented Generation (RAG), Prompt Engineering, Model Evaluation & Safety (bias, hallucination detection)\\\\n* Data Processing: Elasticsearch, Milvus, Data Warehousing, ETL Pipelines\\\\n* Programming: Python, R, SQL\\\\n* Visualization: R Shiny, Power BI, Streamlit, Gradio, Dash\\\\n* Deep Learning Frameworks: TensorFlow, PyTorch, Hugging Face Transformers\\\\n* Cloud: AWS Sagemaker, IBM Watson Studio, Docker, Git, CI/CD Pipelines\\\\n\\\\n**Work Experience:**\\\\n\\\\n* **Data Scientist**, IBM, Dublin (May 2019 \\\\u2013 Present)\\\\n* **Data Analyst**, Voxpro Groups, Cork (Jan 2018 \\\\u2013 Apr 2019)\\\\n* **BI Developer**, Cognizant Technology Solutions, Bengaluru (Jul 2012 \\\\u2013 Aug 2016)\"}\\n\\n----------\\n\\n{\"job_description\": \"Here are the extracted information:\\\\n\\\\n**Job Title:** \\\\nNot explicitly mentioned, but based on the responsibilities, it can be inferred as a **Data Scientist - Machine Learning Engineer** or similar.\\\\n\\\\n**Responsibilities:**\\\\n\\\\n1. Analyze, review and interpret complex data through case-studies\\\\n2. Build and maintain production-ready machine-learning models\\\\n3. Identify drivers of patterns/behaviors in data and explain business trends\\\\n4. Contribute to designing and realizing analytical tools/assets\\\\n5. Engage with subject-matter experts to explore business problems and design solutions\\\\n6. Present analysis and interpretation for operational and business review and planning\\\\n7. Support short and long term operational and strategic business activities through data analysis\\\\n\\\\n**Required/Preferred Skills:**\\\\n\\\\n1. **Education:** Master\\'s degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain\\\\n2. **Technical Skills:**\\\\n\\\\t* Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch)\\\\n\\\\t* Experience in developing and deploying machine learning models in production environments\\\\n3. **Analytical Skills:**\\\\n\\\\t* Strong quantitative analysis and problem-solving skills\\\\n\\\\t* Ability to interpret complex data and identify patterns/behaviors.\"}\\n        '}], 'temperature': 0.7, 'stream': False, 'stop': ['\\nObservation:']}'\n",
      "\u001b[0m\n",
      "\n",
      "2025-06-05 14:13:34,170 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:34,170 - DEBUG - send_request_headers.complete\n",
      "2025-06-05 14:13:34,171 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:34,171 - DEBUG - send_request_body.complete\n",
      "2025-06-05 14:13:34,171 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:35,937 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 05 Jun 2025 13:13:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-europe-west3'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'30000'), (b'x-ratelimit-remaining-requests', b'995'), (b'x-ratelimit-remaining-tokens', b'27711'), (b'x-ratelimit-reset-requests', b'7m9.314999999s'), (b'x-ratelimit-reset-tokens', b'4.577s'), (b'x-request-id', b'req_01jx03ke88ea3rnhpgkjds01e3'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94afed94bec2d745-LHR'), (b'Content-Encoding', b'br'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2025-06-05 14:13:35,938 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-06-05 14:13:35,938 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-06-05 14:13:35,940 - DEBUG - receive_response_body.complete\n",
      "2025-06-05 14:13:35,941 - DEBUG - response_closed.started\n",
      "2025-06-05 14:13:35,942 - DEBUG - response_closed.complete\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: utils.py:336 - RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-a58123b6-bfb6-404f-9278-eb42fa0a9e58\", \"object\": \"chat.completion\", \"created\": 1749129214, \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Here is a professional cover letter that highlights the candidate's strengths, acknowledges and bridges skill or experience gaps, and aligns with the job description and tone:\\n\\nDear Hiring Manager,\\n\\nI am excited to apply for the Data Scientist - Machine Learning Engineer position at [Company Name]. With a strong educational background in Data Science and Analytics, and extensive experience in developing and deploying machine learning models, I am confident that I can make a valuable contribution to your team.\\n\\nAs a highly motivated and detail-oriented data scientist with a Master's degree in Data Science and Analytics from University College Cork, I possess a solid foundation in machine learning, statistics, and programming. My academic background, combined with my industry experience, has equipped me with a unique blend of technical and analytical skills. I am well-versed in AI/ML and deep learning frameworks, including PyTorch, TensorFlow, and Hugging Face Transformers. My proficiency in programming languages such as Python, R, and SQL enables me to effectively work with large datasets and develop efficient data processing pipelines.\\n\\nIn my current role as a Data Scientist at IBM, Dublin, I have gained significant experience in building and maintaining production-ready machine learning models, analyzing complex data, and identifying drivers of patterns and behaviors. I have successfully applied my skills in time series forecasting, NLP, and large language models to drive business value. I am confident that my expertise in machine learning, data processing, and visualization can be leveraged to support short and long-term operational and strategic business activities.\\n\\nWhile my background is strong in machine learning and data analysis, I acknowledge that there may be areas where I can further develop my skills. Specifically, I recognize the importance of staying up-to-date with the latest advancements in the field and expanding my expertise in areas such as retrieval-augmented generation and prompt engineering. I am eager to learn and adapt to new technologies and methodologies, and I am confident that my strong foundation in machine learning and data analysis will enable me to quickly absorb and apply new concepts.\\n\\nThe job responsibilities outlined in the description align closely with my skills and experience. I am confident that my ability to analyze complex data, identify patterns and behaviors, and present insights to stakeholders will enable me to make a significant impact in this role. I am excited about the opportunity to engage with subject-matter experts to explore business problems and design solutions, and I am confident that my strong communication skills will enable me to effectively present analysis and interpretation for operational and business review and planning.\\n\\nThank you for considering my application. I am confident that my technical skills, business acumen, and passion for machine learning and data analysis make me an ideal candidate for this role. I look forward to discussing my application and how I can contribute to the success of [Company Name].\\n\\nSincerely,\\n\\nVikram Bhat\\n\\nThis cover letter:\\n\\n* Highlights the candidate's strengths in machine learning, data analysis, and programming\\n* Acknowledges potential gaps in skills or experience and demonstrates a willingness to learn and adapt\\n* Aligns with the job description and tone, using language and keywords from the job posting\\n* Showcases the candidate's achievements and qualifications, and demonstrates enthusiasm for the role and company.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.177242416, \"prompt_tokens\": 598, \"prompt_time\": 0.02824358, \"completion_tokens\": 633, \"completion_time\": 1.5080683879999999, \"total_tokens\": 1231, \"total_time\": 1.5363119680000001}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_79da0e0073\", \"x_groq\": {\"id\": \"req_01jx03ke88ea3rnhpgkjds01e3\"}}\n",
      "\n",
      "\n",
      "2025-06-05 14:13:35,943 - DEBUG - RAW RESPONSE:\n",
      "{\"id\": \"chatcmpl-a58123b6-bfb6-404f-9278-eb42fa0a9e58\", \"object\": \"chat.completion\", \"created\": 1749129214, \"model\": \"meta-llama/llama-4-scout-17b-16e-instruct\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Here is a professional cover letter that highlights the candidate's strengths, acknowledges and bridges skill or experience gaps, and aligns with the job description and tone:\\n\\nDear Hiring Manager,\\n\\nI am excited to apply for the Data Scientist - Machine Learning Engineer position at [Company Name]. With a strong educational background in Data Science and Analytics, and extensive experience in developing and deploying machine learning models, I am confident that I can make a valuable contribution to your team.\\n\\nAs a highly motivated and detail-oriented data scientist with a Master's degree in Data Science and Analytics from University College Cork, I possess a solid foundation in machine learning, statistics, and programming. My academic background, combined with my industry experience, has equipped me with a unique blend of technical and analytical skills. I am well-versed in AI/ML and deep learning frameworks, including PyTorch, TensorFlow, and Hugging Face Transformers. My proficiency in programming languages such as Python, R, and SQL enables me to effectively work with large datasets and develop efficient data processing pipelines.\\n\\nIn my current role as a Data Scientist at IBM, Dublin, I have gained significant experience in building and maintaining production-ready machine learning models, analyzing complex data, and identifying drivers of patterns and behaviors. I have successfully applied my skills in time series forecasting, NLP, and large language models to drive business value. I am confident that my expertise in machine learning, data processing, and visualization can be leveraged to support short and long-term operational and strategic business activities.\\n\\nWhile my background is strong in machine learning and data analysis, I acknowledge that there may be areas where I can further develop my skills. Specifically, I recognize the importance of staying up-to-date with the latest advancements in the field and expanding my expertise in areas such as retrieval-augmented generation and prompt engineering. I am eager to learn and adapt to new technologies and methodologies, and I am confident that my strong foundation in machine learning and data analysis will enable me to quickly absorb and apply new concepts.\\n\\nThe job responsibilities outlined in the description align closely with my skills and experience. I am confident that my ability to analyze complex data, identify patterns and behaviors, and present insights to stakeholders will enable me to make a significant impact in this role. I am excited about the opportunity to engage with subject-matter experts to explore business problems and design solutions, and I am confident that my strong communication skills will enable me to effectively present analysis and interpretation for operational and business review and planning.\\n\\nThank you for considering my application. I am confident that my technical skills, business acumen, and passion for machine learning and data analysis make me an ideal candidate for this role. I look forward to discussing my application and how I can contribute to the success of [Company Name].\\n\\nSincerely,\\n\\nVikram Bhat\\n\\nThis cover letter:\\n\\n* Highlights the candidate's strengths in machine learning, data analysis, and programming\\n* Acknowledges potential gaps in skills or experience and demonstrates a willingness to learn and adapt\\n* Aligns with the job description and tone, using language and keywords from the job posting\\n* Showcases the candidate's achievements and qualifications, and demonstrates enthusiasm for the role and company.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.177242416, \"prompt_tokens\": 598, \"prompt_time\": 0.02824358, \"completion_tokens\": 633, \"completion_time\": 1.5080683879999999, \"total_tokens\": 1231, \"total_time\": 1.5363119680000001}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_79da0e0073\", \"x_groq\": {\"id\": \"req_01jx03ke88ea3rnhpgkjds01e3\"}}\n",
      "\n",
      "\n",
      "\u001b[92m14:13:35 - LiteLLM:INFO\u001b[0m: utils.py:1211 - Wrapper: Completed Call, calling success_handler\n",
      "2025-06-05 14:13:35,944 - INFO - Wrapper: Completed Call, calling success_handler\n",
      "\u001b[92m14:13:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:655 - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "2025-06-05 14:13:35,945 - INFO - selected model name for cost calculation: groq/meta-llama/llama-4-scout-17b-16e-instruct\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4245 - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "2025-06-05 14:13:35,946 - DEBUG - checking potential_model_names in litellm.model_cost: {'split_model': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'stripped_model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'combined_stripped_model_name': 'groq/meta-llama/llama-4-scout-17b-16e-instruct', 'custom_llm_provider': 'groq'}\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:376 - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.578e-05, completion_tokens_cost_usd_dollar: 0.00021522\n",
      "2025-06-05 14:13:35,947 - DEBUG - Returned custom cost for model=groq/meta-llama/llama-4-scout-17b-16e-instruct - prompt_tokens_cost_usd_dollar: 6.578e-05, completion_tokens_cost_usd_dollar: 0.00021522\n",
      "\u001b[92m14:13:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1124 - response_cost: 0.000281\n",
      "2025-06-05 14:13:35,948 - DEBUG - response_cost: 0.000281\n"
     ]
    }
   ],
   "source": [
    "result = crew.kickoff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "cad5fc40",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"job_description\": \"Here are the extracted information:\\n\\n### Job Title\\nMachine Learning Engineer/Mathematical Data Analyst\\n\\n### Responsibilities\\n\\u2022 Analyze, review, and interpret complex data through case-studies.\\n\\u2022 Build and maintain production-ready machine-learning models.\\n\\u2022 Identify drivers of patterns/behaviors in data to explain business trends.\\n\\u2022 Contribute to the design and realization of analytical tools/assets to identify non-standard claim patterns.\\n\\u2022 Engage with subject-matter experts to explore business problems and design solutions.\\n\\u2022 Present analysis and interpretation for operational and business review and planning.\\n\\u2022 Support short-term and long-term operational and strategic business activities through data use.\\n\\u2022 Develop recommended business solutions through research, analysis, and implementation.\\n\\n### Required/Preferred Skills\\n- Master's degree in Statistics, Physics, Mathematics, Engineering or similar analytic domain with a significant quantitative aspect.\\n- Proficiency in AI/ML and deep learning frameworks (e.g., PyTorch).\\n- Proven track record of developing and deploying machine learning models in production environments.\"}\n"
     ]
    }
   ],
   "source": [
    "print(jd_task.output.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3591e48e",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided resume and job description, here is the comparison of skills and experiences:\n",
      "\n",
      "**Matched Skills:**\n",
      "\n",
      "* Machine Learning & AI:\n",
      "\t+ Supervised & Unsupervised Learning\n",
      "\t+ Time Series Forecasting\n",
      "\t+ NLP\n",
      "\t+ Large Language Models (LLMs)\n",
      "\t+ Retrieval-Augmented Generation (RAG)\n",
      "\t+ Model Evaluation & Safety (bias, hallucination detection) - These skills match with the job description's requirements.\n",
      "* Data Processing:\n",
      "\t+ Elasticsearch and Milvus are mentioned in both the resume and job description, indicating a possible match. However, it is not explicitly stated that these tools were used for the responsibilities listed.\n",
      "* Deep Learning Frameworks:\n",
      "\t+ TensorFlow, PyTorch, and Hugging Face Transformers are all mentioned in the resume's skills section, which align with the job description's required skills.\n",
      "\n",
      "**Missing Skills:**\n",
      "\n",
      "* Statistics\n",
      "* Physics\n",
      "* Engineering\n",
      "\n",
      "These fields are mentioned in the job description as preferred degrees, but not explicitly listed in the resume.\n",
      "\n",
      "**Score:** 0.6 (out of 1.0)\n",
      "\n",
      "The score is based on the matching percentage of skills between the resume and job description. The candidate has a good match with the required skills, but lacks experience or education in statistics, physics, and engineering, which are preferred by the employer.\n"
     ]
    }
   ],
   "source": [
    "print(matching_task.output.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d337d0bb",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"cover_letter\": \"Here is a professional cover letter that highlights the candidate's strengths, acknowledges and bridges skill or experience gaps, and aligns with the job description and tone:\\n\\nDear Hiring Manager,\\n\\nI am excited to apply for the Data Scientist - Machine Learning Engineer position at [Company Name]. With a strong educational background in Data Science and Analytics, and extensive experience in developing and deploying machine learning models, I am confident that I can make a valuable contribution to your team.\\n\\nAs a highly motivated and detail-oriented data scientist with a Master's degree in Data Science and Analytics from University College Cork, I possess a solid foundation in machine learning, statistics, and programming. My academic background, combined with my industry experience, has equipped me with a unique blend of technical and analytical skills. I am well-versed in AI/ML and deep learning frameworks, including PyTorch, TensorFlow, and Hugging Face Transformers. My proficiency in programming languages such as Python, R, and SQL enables me to effectively work with large datasets and develop efficient data processing pipelines.\\n\\nIn my current role as a Data Scientist at IBM, Dublin, I have gained significant experience in building and maintaining production-ready machine learning models, analyzing complex data, and identifying drivers of patterns and behaviors. I have successfully applied my skills in time series forecasting, NLP, and large language models to drive business value. I am confident that my expertise in machine learning, data processing, and visualization can be leveraged to support short and long-term operational and strategic business activities.\\n\\nWhile my background is strong in machine learning and data analysis, I acknowledge that there may be areas where I can further develop my skills. Specifically, I recognize the importance of staying up-to-date with the latest advancements in the field and expanding my expertise in areas such as retrieval-augmented generation and prompt engineering. I am eager to learn and adapt to new technologies and methodologies, and I am confident that my strong foundation in machine learning and data analysis will enable me to quickly absorb and apply new concepts.\\n\\nThe job responsibilities outlined in the description align closely with my skills and experience. I am confident that my ability to analyze complex data, identify patterns and behaviors, and present insights to stakeholders will enable me to make a significant impact in this role. I am excited about the opportunity to engage with subject-matter experts to explore business problems and design solutions, and I am confident that my strong communication skills will enable me to effectively present analysis and interpretation for operational and business review and planning.\\n\\nThank you for considering my application. I am confident that my technical skills, business acumen, and passion for machine learning and data analysis make me an ideal candidate for this role. I look forward to discussing my application and how I can contribute to the success of [Company Name].\\n\\nSincerely,\\n\\nVikram Bhat\\n\\nThis cover letter:\\n\\n* Highlights the candidate's strengths in machine learning, data analysis, and programming\\n* Acknowledges potential gaps in skills or experience and demonstrates a willingness to learn and adapt\\n* Aligns with the job description and tone, using language and keywords from the job posting\\n* Showcases the candidate's achievements and qualifications, and demonstrates enthusiasm for the role and company.\"}\n"
     ]
    }
   ],
   "source": [
    "print(result.raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
